{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbea81f0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-02T08:02:02.844598Z",
     "start_time": "2025-05-02T08:02:00.896492Z"
    }
   },
   "outputs": [],
   "source": [
    "from elasticsearch import Elasticsearch\n",
    "# Buat koneksi ke Elasticsearch\n",
    "es = Elasticsearch(\n",
    "    \"http://34.101.178.71:9200/\",\n",
    "    basic_auth=(\"elastic\", \"elasticpassword\")  # Sesuaikan dengan kredensial Anda\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de0b9722",
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Query untuk mencari dokumen dengan channel = 'news' dan date tertentu\n",
    "query_body = {\n",
    "    \"source\":[\"link_post\"],\n",
    "    \"query\": {\n",
    "        \"bool\": {\n",
    "            \"must_not\": {\n",
    "                \"exists\": {\n",
    "                    \"field\": \"viral_score\"\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "\n",
    "# Tarik data dari index 'news_data'\n",
    "response = es.search(\n",
    "    index=\"news_data\",\n",
    "    body=query_body,\n",
    "    size=10000  # Jumlah maksimal hasil yang ditarik (default hanya 10)\n",
    ")\n",
    "# Ambil hasil dokumen\n",
    "hits = response[\"hits\"][\"hits\"]\n",
    "documents = [doc[\"_source\"] for doc in hits]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3f3ef6c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcc3c083",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-02T08:11:57.315480Z",
     "start_time": "2025-05-02T08:11:57.300182Z"
    }
   },
   "outputs": [],
   "source": [
    "from datetime import datetime, timedelta\n",
    "from chart_generator.functions import *\n",
    "\n",
    "KEYWORDS = ['APBN', 'Anggaran Pendapatan dan Belanja Negara', 'RAPBN', 'Kementerian Keuangan', 'Sri Mulyani', 'Defisit APBN', 'Belanja Negara', 'Pendapatan Negara', 'Pajak', 'Utang Negara', 'Subsidi', 'Transfer Daerah', 'Dana Desa', 'Postur APBN', 'APBN 2024', 'apbn']\n",
    "MAIN_TOPIC = 'APBN'\n",
    "\n",
    "start_date = '2025-04-01'\n",
    "end_date = '2025-05-01'\n",
    "START_DATE = '2025-04-01'\n",
    "END_DATE = '2025-05-01'\n",
    "\n",
    "diff_date = range_date_count(start_date, end_date)\n",
    "prev_start_date = kurangi_tanggal(start_date, diff_date+1)\n",
    "prev_end_date = kurangi_tanggal(end_date, diff_date+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc5b6e70",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-02T07:47:49.743047Z",
     "start_time": "2025-05-02T07:47:49.723198Z"
    }
   },
   "outputs": [],
   "source": [
    "import re\n",
    "print('KEYWORD',KEYWORDS)\n",
    "\n",
    "FILTER_KEYWORD = []\n",
    "for key in KEYWORDS:\n",
    "    keyword = re.sub(f\"[^a-z0-9\\s]\", \" \", key.lower().strip(\" -\"))\n",
    "    FILTER_KEYWORD.append(f\"\"\"SEARCH(lower(a.post_caption), '{keyword}')\"\"\")\n",
    "FILTER_KEYWORD = '(' + ' OR '.join(FILTER_KEYWORD) + ')'\n",
    "FILTER_KEYWORD = FILTER_KEYWORD.replace('\\u2060', '')\n",
    "\n",
    "FILTER_DATE = f\"\"\"a.post_created_at BETWEEN '{start_date}' AND '{end_date}'\"\"\"\n",
    "ALL_FILTER = f\"{FILTER_DATE} AND {FILTER_KEYWORD}\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6894e186",
   "metadata": {},
   "source": [
    "# BQ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04e9de32",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-02T08:17:29.961910Z",
     "start_time": "2025-05-02T08:17:28.810801Z"
    },
    "code_folding": [
     6
    ]
   },
   "outputs": [],
   "source": [
    "from chart_generator.functions import *\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()  \n",
    "BQ = About_BQ(project_id= os.getenv(\"BQ_PROJECT_ID\") ,\n",
    "         credentials_loc= os.getenv(\"BQ_CREDS_LOCATION\")  )\n",
    "def join_issues_with_metrics(data, df_prediction):\n",
    "    \"\"\"\n",
    "    Join the metrics data with the prediction data based on issue mapping.\n",
    "    \n",
    "    Args:\n",
    "        data (list/DataFrame): List of dictionaries or DataFrame containing metrics data\n",
    "        df_prediction (list/DataFrame): List of dictionaries or DataFrame containing prediction data\n",
    "        \n",
    "    Returns:\n",
    "        DataFrame: Joined and aggregated DataFrame\n",
    "    \"\"\"\n",
    "    # Convert to DataFrames if they're lists\n",
    "    if isinstance(data, list):\n",
    "        data_df = pd.DataFrame(data)\n",
    "    else:\n",
    "        data_df = data.copy()\n",
    "        \n",
    "    if isinstance(df_prediction, list):\n",
    "        pred_df = pd.DataFrame(df_prediction)\n",
    "    else:\n",
    "        pred_df = df_prediction.copy()\n",
    "    \n",
    "    # Create an empty results DataFrame to store our joined data\n",
    "    results = []\n",
    "    \n",
    "    # Iterate through each unified issue\n",
    "    for _, row in pred_df.iterrows():\n",
    "        unified_issue = row['unified_issue']\n",
    "        list_issues = row['list_original_issue']\n",
    "        description = row['description']\n",
    "        \n",
    "        # Find all matching issues in data_df\n",
    "        matching_issues = data_df[data_df['issue'].isin(list_issues)]\n",
    "        \n",
    "        if len(matching_issues) > 0:\n",
    "            # Aggregate the metrics\n",
    "            total_issue = matching_issues['total_issue'].sum()\n",
    "            total_reach_score = matching_issues['total_reach_score'].sum()\n",
    "            total_viral_score = matching_issues['total_viral_score'].sum()\n",
    "            \n",
    "            # Calculate weighted averages for percentages\n",
    "            weighted_neg = np.average(\n",
    "                matching_issues['percentage_negative'], \n",
    "                weights=matching_issues['total_issue']\n",
    "            ) if len(matching_issues) > 0 else 0\n",
    "            \n",
    "            weighted_pos = np.average(\n",
    "                matching_issues['percentage_positive'], \n",
    "                weights=matching_issues['total_issue']\n",
    "            ) if len(matching_issues) > 0 else 0\n",
    "            \n",
    "            weighted_neutral = np.average(\n",
    "                matching_issues['percentage_neutral'], \n",
    "                weights=matching_issues['total_issue']\n",
    "            ) if len(matching_issues) > 0 else 0\n",
    "            \n",
    "            # Add to results\n",
    "            results.append({\n",
    "                'unified_issue': unified_issue,\n",
    "                'description': description,\n",
    "                'total_issue': total_issue,\n",
    "                'total_viral_score': total_viral_score,\n",
    "                'total_reach_score':total_reach_score,\n",
    "                'percentage_negative': round(weighted_neg, 2),\n",
    "                'percentage_positive': round(weighted_pos, 2),\n",
    "                'percentage_neutral': round(weighted_neutral, 2),\n",
    "                'list_issue': list_issues\n",
    "            })\n",
    "        else:\n",
    "            # If no matching issues were found, still include the unified issue\n",
    "            # but with zero values for metrics\n",
    "            results.append({\n",
    "                'unified_issue': unified_issue,\n",
    "                'description': description,\n",
    "                'total_issue': 0,\n",
    "                'total_viral_score': 0,\n",
    "                'total_reach_score':0,\n",
    "                'percentage_negative': 0,\n",
    "                'percentage_positive': 0,\n",
    "                'percentage_neutral': 0,\n",
    "                'list_issue': list_issues\n",
    "            })\n",
    "    \n",
    "    # Convert results to DataFrame\n",
    "    results_df = pd.DataFrame(results)\n",
    "    results_df['share_of_voice'] = results_df['total_issue']/results_df['total_issue'].sum()\n",
    "    return results_df\n",
    "\n",
    "def get_references(rf):\n",
    "    # Hitung jumlah unique channel\n",
    "    unique_channels = rf['channel'].nunique()\n",
    "\n",
    "    # Tentukan jumlah top posts per channel berdasarkan jumlah unique channel\n",
    "    if unique_channels <= 2:\n",
    "        # Jika channel <= 2, ambil top 3 untuk setiap channel\n",
    "        n_per_channel = 3\n",
    "    elif unique_channels == 3:\n",
    "        # Jika channel = 3, ambil top 2 untuk setiap channel\n",
    "        n_per_channel = 2\n",
    "    else:\n",
    "        # Jika channel >= 4, ambil top 1 untuk setiap channel\n",
    "        n_per_channel = 1\n",
    "        # Batasi pada 6 channel teratas jika terlalu banyak channel\n",
    "        if unique_channels > 6:\n",
    "            # Dapatkan 6 channel dengan rata-rata reach_score tertinggi\n",
    "            top_channels = rf.groupby('channel')['reach_score'].mean().nlargest(6).index.tolist()\n",
    "            # Filter rf hanya untuk channel-channel tersebut\n",
    "            rf_filtered = rf[rf['channel'].isin(top_channels)]\n",
    "        else:\n",
    "            rf_filtered = rf\n",
    "\n",
    "    # Ambil top n_per_channel posts untuk setiap channel\n",
    "    if unique_channels > 6:\n",
    "        # Jika sudah difilter ke 6 channel teratas\n",
    "        top_posts = rf_filtered.groupby('channel').apply(\n",
    "            lambda x: x.nlargest(n_per_channel, 'reach_score')\n",
    "        ).reset_index(drop=True)\n",
    "    else:\n",
    "        # Jika jumlah channel <= 6\n",
    "        top_posts = rf.groupby('channel').apply(\n",
    "            lambda x: x.nlargest(n_per_channel, 'reach_score')\n",
    "        ).reset_index(drop=True)\n",
    "\n",
    "    # Tampilkan hasil\n",
    "    print(f\"Total posts: {len(top_posts)}\")\n",
    "    return (top_posts[['channel', 'link_post']])\n",
    "\n",
    "def generate_topic_overview(ALL_FILTER, TOPIC, SAVE_PATH = 'PPT'):\n",
    "    # GET DATA\n",
    "    query = f\"\"\"\n",
    "        WITH filtered_posts AS (\n",
    "            SELECT\n",
    "                c.issue,\n",
    "                a.viral_score,\n",
    "                c.sentiment,\n",
    "                a.post_caption,\n",
    "                a.reach_score\n",
    "            FROM medsos.post_analysis a\n",
    "            JOIN medsos.post_category c\n",
    "                ON a.link_post = c.link_post\n",
    "            WHERE\n",
    "                {ALL_FILTER}\n",
    "                AND lower(issue) NOT IN ('not specified')\n",
    "        )\n",
    "\n",
    "        SELECT\n",
    "            issue,\n",
    "            COUNT(issue) AS total_issue,\n",
    "            SUM(viral_score) AS total_viral_score,\n",
    "            SUM(reach_score) AS total_reach_score,\n",
    "            ROUND(100 * SUM(CASE WHEN sentiment = 'negative' THEN 1 ELSE 0 END) / COUNT(*), 2) AS percentage_negative,\n",
    "            ROUND(100 * SUM(CASE WHEN sentiment = 'positive' THEN 1 ELSE 0 END) / COUNT(*), 2) AS percentage_positive,\n",
    "            ROUND(100 * SUM(CASE WHEN sentiment = 'neutral' THEN 1 ELSE 0 END) / COUNT(*), 2) AS percentage_neutral,\n",
    "            ARRAY_AGG(post_caption ORDER BY viral_score DESC LIMIT 3) AS top_post_captions\n",
    "        FROM filtered_posts\n",
    "        GROUP BY issue\n",
    "        ORDER BY total_issue DESC, total_viral_score DESC\n",
    "        LIMIT 40;\"\"\"\n",
    "    data = BQ.to_pull_data(query)\n",
    "\n",
    "    # Summarize Topics\n",
    "    prompt = f\"\"\"\n",
    "    You are a Social Media Analyst Expert. This is the topic about [{TOPIC}]\n",
    "    Your task is to analyze and group similar issues together based on their meaning, then generate a concise and meaningful description based on the provided post captions.\n",
    "\n",
    "    ### Instructions:\n",
    "    1. Identify and **group similar issues** under a single **unified issue name** that best represents the grouped topics.\n",
    "    2. For each unified issue:\n",
    "       - Create a **list_original_issue** containing ONLY the unique original issue names grouped under this unified issue.\n",
    "       - IMPORTANT: After collecting all original issues, REMOVE ALL DUPLICATES before finalizing the list_original_issue.\n",
    "       - Process each string to ensure exact duplicates (including case and spacing) are removed.\n",
    "       - Check for and eliminate semantic duplicates (same meaning but slightly different wording).\n",
    "    3. Use the **top_post_captions** from each issue group to generate a **short and insightful description** summarizing the key discussions.\n",
    "    4. Remove from analysis if you found that the post is not related to the Topic\n",
    "    5. Format the output as **valid JSON**.\n",
    "\n",
    "    ### Data:\n",
    "    {data[['issue', 'top_post_captions']].to_dict(orient='records')}\n",
    "\n",
    "    ### Output Format (JSON):\n",
    "    [\n",
    "      {{\n",
    "        \"unified_issue\": \"<Unified issue name>\",\n",
    "        \"list_original_issue\": [\"<issue 1>\", \"<issue 2>\", \"<issue 3>\"],\n",
    "        \"description\": \"<A short yet meaningful analysis considering key discussion points from the captions, jangan gunakan tanda kutip \\\" tapi gunakan \\`>\"\n",
    "      }}\n",
    "    ]\n",
    "\n",
    "    ### Critical Rules for list_original_issue:\n",
    "    - Before finalizing each list_original_issue, PERFORM AN EXPLICIT DEDUPLICATION check.\n",
    "    - VERIFY that NO DUPLICATE ENTRIES appear in any list_original_issue.\n",
    "    - First remove exact duplicates, then check for semantic duplicates (like \"Student protests against government\" vs \"Student protests against government policies\").\n",
    "    - Each original issue must appear in EXACTLY ONE unified group.\n",
    "    - Double-check the final output to ensure no duplicates remain in any list_original_issue.\n",
    "    \"\"\"\n",
    "\n",
    "    \n",
    "    prediction = call_gemini(prompt)\n",
    "    df_prediction = pd.DataFrame(eval(re.findall(r'\\[.*\\]',prediction, flags=re.I|re.S)[0]))\n",
    "\n",
    "    result_df = join_issues_with_metrics(data, df_prediction)\n",
    "\n",
    "    # Get References\n",
    "    query = f\"\"\"\n",
    "    select c.link_post, c.issue,  reach_score, a.channel from medsos.post_category  c\n",
    "    JOIN medsos.post_analysis a\n",
    "    ON a.link_post = c.link_post\n",
    "    where c.issue in {tuple([j for i in result_df.head(5)['list_issue'] for j in i])}\n",
    "    order by viral_score desc \n",
    "\n",
    "    \"\"\"\n",
    "    list_berita = BQ.to_pull_data(query)\n",
    "    #reach_score,viral_score, influence_score,\n",
    "    references = []\n",
    "    for issues in result_df['list_issue']:\n",
    "        rf = list_berita[list_berita['issue'].isin(issues)]\n",
    "        channels = rf['channel'].nunique()\n",
    "        references.append(get_references(rf).to_dict(orient = 'records'))\n",
    "\n",
    "    result_df['references'] = references\n",
    "\n",
    "    with open(os.path.join(SAVE_PATH,'topic_overview.json'),'w') as f:\n",
    "        for i in result_df.to_dict(orient = 'records'):\n",
    "            f.write(json.dumps(i)+'\\n')\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eefd75c0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-02T08:17:36.823382Z",
     "start_time": "2025-05-02T08:17:31.046795Z"
    },
    "code_folding": [
     4
    ],
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from datetime import datetime, timedelta\n",
    "from chart_generator.functions import *\n",
    "\n",
    "KEYWORDS = ['APBN', 'Anggaran Pendapatan dan Belanja Negara', 'RAPBN', 'Kementerian Keuangan', 'Sri Mulyani', 'Defisit APBN', 'Belanja Negara', 'Pendapatan Negara', 'Pajak', 'Utang Negara', 'Subsidi', 'Transfer Daerah', 'Dana Desa', 'Postur APBN', 'APBN 2024', 'apbn']\n",
    "MAIN_TOPIC = 'APBN'\n",
    "\n",
    "start_date = '2025-04-01'\n",
    "end_date = '2025-05-01'\n",
    "START_DATE = '2025-04-01'\n",
    "END_DATE = '2025-05-01'\n",
    "\n",
    "diff_date = range_date_count(start_date, end_date)\n",
    "prev_start_date = kurangi_tanggal(start_date, diff_date+1)\n",
    "prev_end_date = kurangi_tanggal(end_date, diff_date+1)\n",
    "\n",
    "generate_sentiment_analysis(MAIN_TOPIC, KEYWORDS,START_DATE,END_DATE, '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ebc4b5d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-02T08:10:12.642561Z",
     "start_time": "2025-05-02T08:10:12.344332Z"
    },
    "code_folding": [
     4
    ]
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d728078",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-02T08:08:11.756739Z",
     "start_time": "2025-05-02T08:08:07.733173Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "sentiment_counts, pivot_sentiment = get_sentiment_data(\n",
    "    KEYWORDS=KEYWORDS,\n",
    "    START_DATE=START_DATE,\n",
    "    END_DATE=END_DATE\n",
    ")\n",
    "#breakdown\n",
    "plot_half_donut_sentiment(sentiment_counts, title='Sentiment Distribution',\n",
    "                      save_path = os.path.join(SAVE_PATH,'sentiment_breakdown.png'))\n",
    "\n",
    "#per platform\n",
    "plot_sentiment_by_channel(pivot_sentiment, save_path = os.path.join(SAVE_PATH,'sentiment_by_categories.png'))\n",
    "\n",
    "#summarize\n",
    "summarize(TOPIC,ALL_FILTER,sentiment_counts,pivot_sentiment, SAVE_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "65d21f11",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-02T11:31:30.836666Z",
     "start_time": "2025-05-02T11:31:27.208865Z"
    }
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "from utils.list_of_mentions import get_filtered_mentions\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def generate_topic_overview(TOPIC, KEYWORDS, START_DATE, END_DATE, SAVE_PATH):\n",
    "    url = \"https://moskal-api-v2-924335637775.us-central1.run.app/api/v2/topics-overview\"\n",
    "\n",
    "    headers = {\n",
    "        \"accept\": \"application/json\",\n",
    "        \"Content-Type\": \"application/json\"\n",
    "    }\n",
    "\n",
    "    payload = {\n",
    "        \"keywords\": KEYWORDS,\n",
    "        \"sentiment\": [\"positive\", \"negative\", \"neutral\"],\n",
    "        \"date_filter\": \"custom\",\n",
    "        \"custom_start_date\": START_DATE,\n",
    "        \"custom_end_date\": END_DATE,\n",
    "        \"owner_id\": \"5\",\n",
    "        \"project_name\": TOPIC\n",
    "    }\n",
    "\n",
    "    response = requests.post(url, headers=headers, json=payload)\n",
    "\n",
    "    df_topics = pd.DataFrame(response.json())\n",
    "\n",
    "    df_topics = df_topics.rename(columns = {'viral_score':'total_viral_score',\n",
    "                            'reach_score':'total_reach_score',\n",
    "                            'total_posts':'total_issue'})\n",
    "\n",
    "    df_topics['percentage_negative'] = df_topics['negative']/df_topics['total_issue']*100\n",
    "    df_topics['percentage_positive'] = df_topics['positive']/df_topics['total_issue']*100\n",
    "    df_topics['percentage_neutral'] = df_topics['neutral']/df_topics['total_issue']*100\n",
    "\n",
    "\n",
    "    df_top_final = df_topics[:5]\n",
    "    list_references = []\n",
    "    for list_issue in df_top_final['list_issue']:\n",
    "        references = get_filtered_mentions(\n",
    "            keywords=list_issue,\n",
    "            start_date=START_DATE,\n",
    "            end_date=END_DATE,\n",
    "            source=[\"channel\",'link_post'],\n",
    "            sort_type='popular',\n",
    "            page=1,\n",
    "            page_size=5\n",
    "        ) \n",
    "        list_references.append(references['data'])\n",
    "    df_top_final['references'] = list_references\n",
    "\n",
    "    with open(os.path.join(SAVE_PATH,'topic_overview.json'),'w') as f:\n",
    "        for i in df_top_final.to_dict(orient = 'records'):\n",
    "            f.write(json.dumps(i)+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "97516abc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-02T11:31:52.014036Z",
     "start_time": "2025-05-02T11:31:37.797417Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "V2\n",
      "-------------->GEMINI_CREDS_LOCATION: skilled-compass.json\n",
      "-------------->GEMINI_PROJECT_ID: paper-ds-production\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime, timedelta\n",
    "from chart_generator.functions import *\n",
    "\n",
    "KEYWORDS = ['APBN', 'Anggaran Pendapatan dan Belanja Negara', 'RAPBN', 'Kementerian Keuangan', 'Sri Mulyani', 'Defisit APBN', 'Belanja Negara', 'Pendapatan Negara', 'Pajak', 'Utang Negara', 'Subsidi', 'Transfer Daerah', 'Dana Desa', 'Postur APBN', 'APBN 2024', 'apbn']\n",
    "MAIN_TOPIC = 'APBN'\n",
    "\n",
    "start_date = '2025-04-01'\n",
    "end_date = '2025-05-01'\n",
    "START_DATE = '2025-04-01'\n",
    "END_DATE = '2025-05-01'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4f10bd51",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-02T11:32:09.897940Z",
     "start_time": "2025-05-02T11:31:57.358680Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Aril Indra Permana\\AppData\\Local\\Temp\\ipykernel_68900\\685608068.py:50: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_top_final['references'] = list_references\n"
     ]
    }
   ],
   "source": [
    "generate_topic_overview(MAIN_TOPIC, KEYWORDS, START_DATE, END_DATE, '')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8582364",
   "metadata": {},
   "source": [
    "# REKOMENDASI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03320a4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import pandas as pd\n",
    "import re\n",
    "from typing import Dict, List, Any, Union\n",
    "\n",
    "# Import utilitas dari paket utils (sesuaikan dengan environment Anda)\n",
    "from chart_generator.functions import call_gemini\n",
    "\n",
    "def generate_strategic_recommendations(TOPIC, START_DATE, END_DATE, SAVE_PATH):\n",
    "    \"\"\"\n",
    "    Membuat rekomendasi strategis berdasarkan analisis data dari file JSON dan CSV yang disimpan\n",
    "    di SAVE_PATH.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    TOPIC : str\n",
    "        Topik utama yang dianalisis\n",
    "    START_DATE : str\n",
    "        Tanggal awal periode analisis\n",
    "    END_DATE : str\n",
    "        Tanggal akhir periode analisis\n",
    "    SAVE_PATH : str\n",
    "        Path direktori tempat semua file analisis disimpan\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    List[Dict]\n",
    "        Daftar rekomendasi strategis dalam format JSON\n",
    "    \"\"\"\n",
    "    \n",
    "    SAVE_PATH = os.path.join('REPORT', topic, f\"{start_date} - {end_date}\")\n",
    "    \n",
    "    # Inisialisasi dictionary untuk menyimpan semua data\n",
    "    data = {}\n",
    "    \n",
    "    # 1. Load sentiment_breakdown.json\n",
    "    try:\n",
    "        with open(os.path.join(SAVE_PATH, 'sentiment_breakdown.json'), 'r') as f:\n",
    "            data['sentiment_counts'] = json.load(f)\n",
    "        print(\"Loaded sentiment breakdown data\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading sentiment breakdown: {e}\")\n",
    "        data['sentiment_counts'] = {}\n",
    "    \n",
    "    # 2. Load sentiment_by_categories.json\n",
    "    try:\n",
    "        with open(os.path.join(SAVE_PATH, 'sentiment_by_categories.json'), 'r') as f:\n",
    "            data['pivot_sentiment'] = json.load(f)\n",
    "        print(\"Loaded sentiment by categories data\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading sentiment by categories: {e}\")\n",
    "        data['pivot_sentiment'] = []\n",
    "    \n",
    "    # 3. Load presence_score_analysis.json\n",
    "    try:\n",
    "        with open(os.path.join(SAVE_PATH, 'presence_score_analysis.json'), 'r') as f:\n",
    "            data['presence_score_analysis'] = json.load(f)\n",
    "        print(\"Loaded presence score analysis\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading presence score analysis: {e}\")\n",
    "        data['presence_score_analysis'] = {}\n",
    "    \n",
    "    # 4. Load sentiment_analysis.json\n",
    "    try:\n",
    "        with open(os.path.join(SAVE_PATH, 'sentiment_analysis.json'), 'r') as f:\n",
    "            data['sentiment_analysis'] = json.load(f)\n",
    "        print(\"Loaded sentiment analysis\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading sentiment analysis: {e}\")\n",
    "        data['sentiment_analysis'] = {}\n",
    "    \n",
    "    # 5. Load popular_mentions.csv\n",
    "    try:\n",
    "        data['popular_mentions'] = pd.read_csv(os.path.join(SAVE_PATH, 'popular_mentions.csv')).to_dict(orient='records')[:10]  # Limit to top 10\n",
    "        print(\"Loaded popular mentions data\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading popular mentions: {e}\")\n",
    "        data['popular_mentions'] = []\n",
    "    \n",
    "    # 6. Load KOL data\n",
    "    try:\n",
    "        kol_data = []\n",
    "        with open(os.path.join(SAVE_PATH, 'kol.json'), 'r') as f:\n",
    "            for line in f:\n",
    "                kol_data.append(json.loads(line.strip()))\n",
    "        data['kol_data'] = kol_data[:10]  # Limit to top 10\n",
    "        print(\"Loaded KOL data\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading KOL data: {e}\")\n",
    "        data['kol_data'] = []\n",
    "    \n",
    "    # 7. Load topic_overview.json\n",
    "    try:\n",
    "        topic_data = []\n",
    "        with open(os.path.join(SAVE_PATH, 'topic_overview.json'), 'r') as f:\n",
    "            for line in f:\n",
    "                topic_data.append(json.loads(line.strip()))\n",
    "        data['top_entities'] = topic_data[:15]  # Limit to top 15\n",
    "        print(\"Loaded topic overview data\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading topic overview: {e}\")\n",
    "        data['top_entities'] = []\n",
    "    \n",
    "    # Extract platform data from sentiment_by_categories.json for list of top platforms\n",
    "    top_platforms = []\n",
    "    if data['pivot_sentiment']:\n",
    "        # Sort by total_mentions and get top 5 platforms\n",
    "        sorted_platforms = sorted(data['pivot_sentiment'], key=lambda x: x.get('total_mentions', 0), reverse=True)\n",
    "        top_platforms = [p.get('channel', '') for p in sorted_platforms[:5] if p.get('channel')]\n",
    "    \n",
    "    # Build the prompt\n",
    "    prompt = f\"\"\"\n",
    "    You are a senior digital communications and media strategy expert. Analyze the following comprehensive data about [{TOPIC}] for the period [{START_DATE}] to [{END_DATE}] and provide strategic recommendations.\n",
    "\n",
    "    ## SENTIMENT DATA\n",
    "    \n",
    "    Overall sentiment distribution:\n",
    "    {data['sentiment_counts']}\n",
    "    \n",
    "    Sentiment by platform/channel:\n",
    "    {data['pivot_sentiment'][:5] if data['pivot_sentiment'] else \"No data available\"}\n",
    "    \"\"\"\n",
    "    \n",
    "    # Add presence score analysis if available\n",
    "    if data['presence_score_analysis']:\n",
    "        prompt += f\"\"\"\n",
    "    ## PRESENCE SCORE ANALYSIS\n",
    "    \n",
    "    Overall presence score trends:\n",
    "    {data['presence_score_analysis']}\n",
    "    \"\"\"\n",
    "    \n",
    "    # Add sentiment analysis if available\n",
    "    if data['sentiment_analysis']:\n",
    "        prompt += f\"\"\"\n",
    "    ## DETAILED SENTIMENT ANALYSIS\n",
    "    \n",
    "    In-depth sentiment analysis:\n",
    "    {data['sentiment_analysis']}\n",
    "    \"\"\"\n",
    "    \n",
    "    # Add popular mentions if available\n",
    "    if data['popular_mentions']:\n",
    "        prompt += f\"\"\"\n",
    "    ## POPULAR MENTIONS\n",
    "    \n",
    "    Sample of popular posts (top shown):\n",
    "    {data['popular_mentions'][:3]}\n",
    "    \"\"\"\n",
    "    \n",
    "    # Add top entities if available\n",
    "    if data['top_entities']:\n",
    "        prompt += f\"\"\"\n",
    "    ## TOP ENTITIES/TOPICS\n",
    "    \n",
    "    Most discussed entities or subtopics:\n",
    "    {data['top_entities'][:10]}\n",
    "    \"\"\"\n",
    "    \n",
    "    # Add KOL data if available\n",
    "    if data['kol_data']:\n",
    "        prompt += f\"\"\"\n",
    "    ## KEY OPINION LEADERS\n",
    "    \n",
    "    Top influencers discussing this topic:\n",
    "    {data['kol_data'][:5]}\n",
    "    \"\"\"\n",
    "    \n",
    "    # Add list of top platforms\n",
    "    prompt += f\"\"\"\n",
    "    ## TOP PLATFORMS\n",
    "    \n",
    "    The most active platforms for this topic are:\n",
    "    {top_platforms if top_platforms else \"Data not available\"}\n",
    "    \"\"\"\n",
    "    \n",
    "    # Instruksi dan format output\n",
    "    prompt += f\"\"\"\n",
    "    ## TASK\n",
    "    \n",
    "    Based on all the data above, provide strategic communication recommendations for managing and improving the discourse around [{TOPIC}].\n",
    "\n",
    "    Format your response as a JSON array with EXACTLY 4 recommendation categories (no more, no less), each with:\n",
    "    - \"title\": A clear, specific category description focusing on communication strategy (limit to 10 words)\n",
    "    - \"actions\": An array of EXACTLY 3 specific, actionable steps (each 1-2 sentences long)\n",
    "\n",
    "    The recommendations should be highly specific to {TOPIC} and directly address the sentiment, trends, and key concerns found in the data. DO NOT provide generic advice.\n",
    "\n",
    "    Each recommendation should be concrete, implementable, and directly tied to the data insights provided.\n",
    "\n",
    "    THE OUTPUT MUST BE VALID JSON. Use this exact format:\n",
    "    [\n",
    "      {{\n",
    "        \"title\": \"Strategy Category Title\",\n",
    "        \"actions\": [\n",
    "          \"Specific actionable step one with clear direction.\",\n",
    "          \"Specific actionable step two with clear direction.\",\n",
    "          \"Specific actionable step three with clear direction.\"\n",
    "        ]\n",
    "      }},\n",
    "      ...\n",
    "    ]\n",
    "\n",
    "    IMPORTANT: Ensure your output is ONLY the properly formatted JSON with no additional text, markdown, or explanatory content. The JSON array must contain EXACTLY 4 recommendation objects. Each recommendation must have EXACTLY 3 actions.\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"Prompt generated, calling LLM...\")\n",
    "    \n",
    "    try:\n",
    "        # Call LLM to generate recommendations\n",
    "        recommendations_text = call_gemini(prompt)\n",
    "        \n",
    "        # Try to parse JSON directly\n",
    "        try:\n",
    "            recommendations = json.loads(recommendations_text)\n",
    "        except json.JSONDecodeError:\n",
    "            # Use regex as fallback if direct parsing fails\n",
    "            print(\"Direct JSON parsing failed, trying regex extraction...\")\n",
    "            json_pattern = re.compile(r'\\[\\s*\\{.*\\}\\s*\\]', re.DOTALL)\n",
    "            json_match = json_pattern.search(recommendations_text)\n",
    "            \n",
    "            if json_match:\n",
    "                json_str = json_match.group(0)\n",
    "                recommendations = json.loads(json_str)\n",
    "            else:\n",
    "                # Last-resort fallback parsing for valid portions\n",
    "                print(\"Regex extraction failed, trying fallback parsing...\")\n",
    "                title_pattern = re.compile(r'\"title\":\\s*\"([^\"]+)\"')\n",
    "                actions_pattern = re.compile(r'\"actions\":\\s*\\[(.*?)\\]', re.DOTALL)\n",
    "                action_item_pattern = re.compile(r'\"([^\"]+)\"')\n",
    "                \n",
    "                recommendations = []\n",
    "                \n",
    "                # Find all title matches\n",
    "                title_matches = title_pattern.finditer(recommendations_text)\n",
    "                for title_match in title_matches:\n",
    "                    title = title_match.group(1)\n",
    "                    # Find corresponding actions\n",
    "                    actions_match = actions_pattern.search(recommendations_text, title_match.end())\n",
    "                    if actions_match:\n",
    "                        actions_text = actions_match.group(1)\n",
    "                        actions = [m.group(1) for m in action_item_pattern.finditer(actions_text)]\n",
    "                        if actions:\n",
    "                            recommendations.append({\"title\": title, \"actions\": actions})\n",
    "                \n",
    "        # Save the recommendations to file\n",
    "        with open(os.path.join(SAVE_PATH, 'recommendations.json'), 'w') as f:\n",
    "            json.dump(recommendations, f, indent=2)\n",
    "            \n",
    "        print(f\"Recommendations generated and saved to {os.path.join(SAVE_PATH, 'recommendations.json')}\")\n",
    "        return recommendations\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error generating recommendations: {e}\")\n",
    "        # Return fallback recommendations\n",
    "        fallback_recommendations = [\n",
    "            {\n",
    "                \"title\": \"Error Processing Data\",\n",
    "                \"actions\": [\n",
    "                    \"Review input data files for completeness and formatting issues.\",\n",
    "                    \"Check LLM connection and retry with simplified prompt.\",\n",
    "                    \"Consider manual analysis if automation continues to fail.\"\n",
    "                ]\n",
    "            }\n",
    "        ]\n",
    "        \n",
    "        with open(os.path.join(SAVE_PATH, 'recommendations_error.json'), 'w') as f:\n",
    "            json.dump(fallback_recommendations, f, indent=2)\n",
    "            \n",
    "        return fallback_recommendations\n",
    "\n",
    "# Contoh penggunaan:\n",
    "if __name__ == \"__main__\":\n",
    "    recommendations = generate_strategic_recommendations(\n",
    "        TOPIC=\"apbn\",\n",
    "        START_DATE=\"2025-04-01\",\n",
    "        END_DATE=\"2025-05-01\"\n",
    "    )\n",
    "    \n",
    "    print(json.dumps(recommendations, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3d82393",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "193236f4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac23406b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db590d64",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24b42f35",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ed248b4b",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# ES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5648e2a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-01T16:58:33.942262Z",
     "start_time": "2025-05-01T16:58:33.915469Z"
    },
    "code_folding": [
     14,
     41,
     68,
     134,
     227
    ],
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "from typing import Dict, List, Optional, Union, Any\n",
    "from chart_generator.functions import *\n",
    "from dotenv import load_dotenv\n",
    "from elasticsearch import Elasticsearch\n",
    "load_dotenv()  \n",
    "\n",
    "es = Elasticsearch(os.getenv('ES_HOST',\"http://34.101.178.71:9200/\"),\n",
    "    basic_auth=(os.getenv(\"ES_USERNAME\",\"elastic\"),os.getenv('ES_PASSWORD',\"elasticpassword\"))  # Sesuaikan dengan kredensial Anda\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "211eebd4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-02T03:19:17.828490Z",
     "start_time": "2025-05-02T03:19:17.682702Z"
    },
    "code_folding": [
     13
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "from typing import Dict, List, Optional, Union, Any\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "from elasticsearch import Elasticsearch\n",
    "load_dotenv()  \n",
    "\n",
    "es = Elasticsearch(os.getenv('ES_HOST',\"http://34.101.178.71:9200/\"),\n",
    "    basic_auth=(os.getenv(\"ES_USERNAME\",\"elastic\"),os.getenv('ES_PASSWORD',\"elasticpassword\"))  # Sesuaikan dengan kredensial Anda\n",
    "        )\n",
    "\n",
    "def get_data(KEYWORDS, START_DATE, END_DATE):\n",
    "    \"\"\"\n",
    "    Menghitung presence score dari data Elasticsearch menggunakan aggregation.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    KEYWORDS : List[str]\n",
    "        Daftar keyword untuk filter\n",
    "    START_DATE : str\n",
    "        Tanggal awal periode (YYYY-MM-DD)\n",
    "    END_DATE : str\n",
    "        Tanggal akhir periode (YYYY-MM-DD)\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    pd.DataFrame\n",
    "        DataFrame dengan kolom date dan presence_score\n",
    "    \"\"\"\n",
    "\n",
    "    # Definisikan semua channel dan indeks\n",
    "    default_channels = ['reddit', 'youtube', 'linkedin', 'twitter', \n",
    "                        'tiktok', 'instagram', 'facebook', 'news', 'threads']\n",
    "    indices = [f\"{ch}_data\" for ch in default_channels]\n",
    "    \n",
    "    # Build keyword filter \n",
    "    keyword_conditions = []\n",
    "    for kw in KEYWORDS:\n",
    "        keyword_conditions.append({\"match\": {\"post_caption\": {\"query\": kw, \"operator\": \"AND\"}}})\n",
    "        keyword_conditions.append({\"match\": {\"issue\": {\"query\": kw, \"operator\": \"AND\"}}})\n",
    "    \n",
    "    keyword_filter = {\n",
    "        \"bool\": {\n",
    "            \"should\": keyword_conditions,\n",
    "            \"minimum_should_match\": 1\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # Bangun query dengan date_histogram aggregation\n",
    "    # untuk mendapatkan total_mentions, total_reach, dan total_engagement per hari\n",
    "    query = {\n",
    "        \"size\": 0,\n",
    "        \"query\": {\n",
    "            \"bool\": {\n",
    "                \"must\": [\n",
    "                    {\n",
    "                        \"range\": {\n",
    "                            \"post_created_at\": {\n",
    "                                \"gte\": START_DATE,\n",
    "                                \"lte\": END_DATE\n",
    "                            }\n",
    "                        }\n",
    "                    },\n",
    "                    keyword_filter\n",
    "                ]\n",
    "            }\n",
    "        },\n",
    "        \"aggs\": {\n",
    "            \"data_per_day\": {\n",
    "                \"date_histogram\": {\n",
    "                    \"field\": \"post_created_at\",\n",
    "                    \"calendar_interval\": \"day\",\n",
    "                    \"format\": \"yyyy-MM-dd\",\n",
    "                    \"min_doc_count\": 0,\n",
    "                    \"extended_bounds\": {\n",
    "                        \"min\": START_DATE,\n",
    "                        \"max\": END_DATE\n",
    "                    }\n",
    "                },\n",
    "                \"aggs\": {\n",
    "                    \"total_reach\": {\n",
    "                        \"sum\": {\n",
    "                            \"field\": \"reach_score\"\n",
    "                        }\n",
    "                    },\n",
    "                    \"total_engagement\": {\n",
    "                        \"sum\": {\n",
    "                            \"script\": {\n",
    "                                \"source\": \"\"\"\n",
    "                                    def likes = doc.containsKey('likes') && !doc['likes'].empty ? doc['likes'].value : 0;\n",
    "                                    def shares = doc.containsKey('shares') && !doc['shares'].empty ? doc['shares'].value : 0;\n",
    "                                    def comments = doc.containsKey('comments') && !doc['comments'].empty ? doc['comments'].value : 0;\n",
    "                                    def favorites = doc.containsKey('favorites') && !doc['favorites'].empty ? doc['favorites'].value : 0;\n",
    "                                    def views = doc.containsKey('views') && !doc['views'].empty ? doc['views'].value : 0;\n",
    "                                    def retweets = doc.containsKey('retweets') && !doc['retweets'].empty ? doc['retweets'].value : 0;\n",
    "                                    def replies = doc.containsKey('replies') && !doc['replies'].empty ? doc['replies'].value : 0;\n",
    "                                    def reposts = doc.containsKey('reposts') && !doc['reposts'].empty ? doc['reposts'].value : 0;\n",
    "                                    def votes = doc.containsKey('votes') && !doc['votes'].empty ? doc['votes'].value : 0;\n",
    "                                    \n",
    "                                    return likes + shares + comments + favorites + views + retweets + replies + reposts + votes;\n",
    "                                \"\"\"\n",
    "                            }\n",
    "                        }\n",
    "                    }\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # Execute query\n",
    "    response = es.search(\n",
    "        index=\",\".join(indices),\n",
    "        body=query\n",
    "    )\n",
    "    \n",
    "    # Ekstrak data dari respons\n",
    "    buckets = response[\"aggregations\"][\"data_per_day\"][\"buckets\"]\n",
    "    \n",
    "    # Siapkan data untuk DataFrame\n",
    "    topic_data = []\n",
    "    for bucket in buckets:\n",
    "        date = bucket[\"key_as_string\"]\n",
    "        total_mentions = bucket[\"doc_count\"]\n",
    "        total_reach = bucket[\"total_reach\"][\"value\"]\n",
    "        total_engagement = bucket[\"total_engagement\"][\"value\"]\n",
    "        \n",
    "        topic_data.append({\n",
    "            \"date\": date,\n",
    "            \"total_mentions\": total_mentions,\n",
    "            \"total_reach\": total_reach,\n",
    "            \"total_engagement\": total_engagement\n",
    "        })\n",
    "    \n",
    "    # Konversi ke DataFrame\n",
    "    df_topic_data = pd.DataFrame(topic_data)\n",
    "    \n",
    "    # Jika tidak ada data, kembalikan DataFrame kosong\n",
    "    if df_topic_data.empty:\n",
    "        return pd.DataFrame(columns=[\"date\", \"presence_score\"])\n",
    "    \n",
    "    # Hitung nilai maksimum\n",
    "    max_mentions = df_topic_data[\"total_mentions\"].max()\n",
    "    max_reach = df_topic_data[\"total_reach\"].max()\n",
    "    max_engagement = df_topic_data[\"total_engagement\"].max()\n",
    "    \n",
    "    # Hitung presence_score\n",
    "    df_topic_data[\"presence_score\"] = df_topic_data.apply(\n",
    "        lambda row: round(\n",
    "            ((row[\"total_mentions\"] / max_mentions if max_mentions else 0) * 40) +\n",
    "            ((row[\"total_reach\"] / max_reach if max_reach else 0) * 40) +\n",
    "            ((row[\"total_engagement\"] / max_engagement if max_engagement else 0) * 20),\n",
    "            2\n",
    "        ),\n",
    "        axis=1\n",
    "    )\n",
    "    \n",
    "    # Pilih kolom yang diperlukan dan urutkan berdasarkan tanggal\n",
    "    result_df = df_topic_data[[\"date\", \"presence_score\"]].sort_values(by=\"date\")\n",
    "    \n",
    "    # Pastikan date dalam format string\n",
    "    result_df[\"date\"] = result_df[\"date\"].astype(str)\n",
    "    \n",
    "    return result_df\n",
    "\n",
    "# Contoh penggunaan:\n",
    "if __name__ == \"__main__\":\n",
    "    # Definisikan parameter\n",
    "    KEYWORDS = ['APBN', 'Anggaran Pendapatan dan Belanja Negara', 'RAPBN', 'Kementerian Keuangan', \n",
    "               'Sri Mulyani', 'Defisit APBN', 'Belanja Negara', 'Pendapatan Negara', 'Pajak', \n",
    "               'Utang Negara', 'Subsidi', 'Transfer Daerah', 'Dana Desa', 'Postur APBN', 'APBN 2024', 'apbn']\n",
    "    \n",
    "    START_DATE = '2025-04-01'\n",
    "    END_DATE = '2025-05-01'\n",
    "    \n",
    "    # Mendapatkan data presence score\n",
    "    presence_score = get_data(\n",
    "        KEYWORDS=KEYWORDS,\n",
    "        START_DATE=START_DATE,\n",
    "        END_DATE=END_DATE\n",
    "    )\n",
    "    \n",
    "    # Tampilkan hasil\n",
    "    if not presence_score.empty:\n",
    "        print(presence_score.head(10))\n",
    "    else:\n",
    "        print(\"No data found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "532c0cc8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-02T03:19:58.473142Z",
     "start_time": "2025-05-02T03:19:58.467017Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "high_presence_date = presence_score.sort_values('presence_score', ascending = False)['date'].to_list()[0]\n",
    "high_presence_date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14504505",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-02T03:20:08.054583Z",
     "start_time": "2025-05-02T03:20:08.027947Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from utils.list_of_mentions import get_filtered_mentions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ef0f6d4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-02T03:21:27.635587Z",
     "start_time": "2025-05-02T03:21:19.345289Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "query = f\"\"\"\n",
    "WITH \n",
    "metric_data AS (\n",
    "    SELECT \n",
    "        sentiment, \n",
    "        a.channel, \n",
    "        COUNT(*) AS total_mentions\n",
    "    FROM medsos.post_analysis a\n",
    "    JOIN medsos.post_category c\n",
    "    ON a.link_post = c.link_post\n",
    "    WHERE {FILTER_KEYWORD}\n",
    "    AND a.post_created_at BETWEEN '{high_presence_date} 00:00:00' AND '{high_presence_date} 29:59:59'\n",
    "    GROUP BY 1, 2\n",
    "),\n",
    "post_data AS (\n",
    "    SELECT * from (select\n",
    "        a.post_caption, \n",
    "        a.channel, \n",
    "        c.issue, \n",
    "        sentiment,\n",
    "        reach_score, \n",
    "        viral_score, \n",
    "        (COALESCE(likes, 0) + COALESCE(shares, 0) + COALESCE(comments, 0) + COALESCE(favorites, 0)\n",
    "         + COALESCE(views, 0) + COALESCE(retweets, 0) + COALESCE(replies, 0) \n",
    "         + COALESCE(reposts, 0) + COALESCE(votes, 0)) AS engagement,\n",
    "        a.link_post\n",
    "    FROM medsos.post_analysis a\n",
    "    JOIN medsos.post_category c\n",
    "    ON a.link_post = c.link_post\n",
    "    WHERE {FILTER_KEYWORD}\n",
    "    AND a.post_created_at BETWEEN '{high_presence_date} 00:00:00' AND '{high_presence_date} 29:59:59'\n",
    "    AND LOWER(c.issue) NOT IN ('not specified'))\n",
    "    order by (reach_score + viral_score + engagement) desc\n",
    "    limit 100\n",
    ")\n",
    "-- Gabungkan hasil dari kedua CTE dengan UNION ALL\n",
    "SELECT \n",
    "    'metrics' AS data_type,\n",
    "    sentiment,\n",
    "    channel,\n",
    "    total_mentions,\n",
    "    NULL AS post_caption,\n",
    "    NULL AS issue,\n",
    "    NULL AS reach_score,\n",
    "    NULL AS viral_score,\n",
    "    NULL AS engagement,\n",
    "    NULL AS link_post\n",
    "FROM metric_data\n",
    "\n",
    "UNION ALL\n",
    "\n",
    "SELECT \n",
    "    'posts' AS data_type,\n",
    "    sentiment,\n",
    "    channel,\n",
    "    NULL AS total_mentions,\n",
    "    post_caption,\n",
    "    issue,\n",
    "    reach_score,\n",
    "    viral_score,\n",
    "    engagement,\n",
    "    link_post\n",
    "FROM post_data\n",
    "ORDER BY data_type, \n",
    "    CASE WHEN data_type = 'posts' THEN (reach_score + viral_score + engagement) END DESC\n",
    "\"\"\"\n",
    "\n",
    "# Eksekusi query gabungan\n",
    "combined_results = BQ.to_pull_data(query)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfe09008",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-02T03:21:33.184452Z",
     "start_time": "2025-05-02T03:21:33.166079Z"
    },
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "combined_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c45a696c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-02T03:22:38.121356Z",
     "start_time": "2025-05-02T03:22:38.110358Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Pisahkan hasil berdasarkan data_type\n",
    "metric_presence = combined_results[combined_results['data_type'] == 'metrics']\n",
    "sample_post_presence = combined_results[combined_results['data_type'] == 'posts']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3187f9a9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-02T03:22:47.927172Z",
     "start_time": "2025-05-02T03:22:47.910960Z"
    },
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "metric_presence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7443c86e",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d1385b6",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "760c920e",
   "metadata": {
    "code_folding": [
     6
    ],
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87b002e7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-02T04:03:19.289674Z",
     "start_time": "2025-05-02T04:03:18.884124Z"
    },
    "code_folding": [
     6
    ],
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d3fc15b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-02T04:03:24.822876Z",
     "start_time": "2025-05-02T04:03:24.812031Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70e65732",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-02T04:03:38.639025Z",
     "start_time": "2025-05-02T04:03:38.624851Z"
    },
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "combined_results[combined_results[\"data_type\"] == \"posts\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10954541",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-02T04:10:17.372986Z",
     "start_time": "2025-05-02T04:10:17.331212Z"
    },
    "code_folding": [
     15,
     50,
     111,
     264,
     486
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "from typing import Dict, List, Optional, Union, Any\n",
    "from chart_generator.functions import *\n",
    "from dotenv import load_dotenv\n",
    "from elasticsearch import Elasticsearch\n",
    "\n",
    "load_dotenv()  \n",
    "\n",
    "es = Elasticsearch(os.getenv('ES_HOST',\"http://34.101.178.71:9200/\"),\n",
    "    basic_auth=(os.getenv(\"ES_USERNAME\",\"elastic\"),os.getenv('ES_PASSWORD',\"elasticpassword\"))  # Sesuaikan dengan kredensial Anda\n",
    "        )\n",
    "\n",
    "\n",
    "def plot_donut_score(score, title=\"How much attention\\na topic or figure gets\", save_path=None):\n",
    "    \"\"\"\n",
    "    Create a donut chart with score (0-100) in the center.\n",
    "    \n",
    "    Parameters:\n",
    "        score (float): Score between 0 and 100\n",
    "        title (str): Text shown below the chart\n",
    "        save_path (str): Optional file path to save the chart\n",
    "    \"\"\"\n",
    "    # Colors\n",
    "    main_color = '#1a73e8'\n",
    "    bg_color = '#e0e0e0'\n",
    "\n",
    "    # Create figure\n",
    "    fig, ax = plt.subplots(figsize=(3, 3))\n",
    "    ax.pie([score, 100 - score],\n",
    "           startangle=90,\n",
    "           colors=[main_color, bg_color],\n",
    "           wedgeprops=dict(width=0.42),\n",
    "           counterclock=False)\n",
    "\n",
    "    # Center text\n",
    "    ax.text(0, 0, f\"{int(round(score))}\", ha='center', va='center', fontsize=40, color='#333333')\n",
    "\n",
    "    # Title below the donut\n",
    "    plt.text(0, -1.3, title, ha='center', va='center', fontsize=12, color='#757575')\n",
    "\n",
    "    # Remove all axes\n",
    "    ax.set_aspect('equal')\n",
    "    plt.axis('off')\n",
    "    plt.tight_layout()\n",
    "\n",
    "    if save_path:\n",
    "        plt.savefig(save_path, bbox_inches='tight', dpi=150, transparent=True)\n",
    "\n",
    "def plot_presence_score_trend(data, title=\"Your presence score\", save_path=None,\n",
    "                              show_dots=True, color_theme='#1a73e8'):\n",
    "    df = pd.DataFrame(data)\n",
    "    df['date'] = pd.to_datetime(df['date'])\n",
    "    df = df.sort_values('date')\n",
    "\n",
    "    x = df['date']\n",
    "    y = df['presence_score'].values\n",
    "\n",
    "    # Smooth\n",
    "    def smooth(x, y):\n",
    "        if len(x) < 4:\n",
    "            return x, y\n",
    "        x_numeric = np.arange(len(x))\n",
    "        spl = make_interp_spline(x_numeric, y, k=3)\n",
    "        xnew = np.linspace(x_numeric.min(), x_numeric.max(), 300)\n",
    "        ynew = spl(xnew)\n",
    "        x_interp = np.interp(xnew, x_numeric, x.astype(np.int64) // 10**9)\n",
    "        x_interp = pd.to_datetime(x_interp, unit='s')\n",
    "        return x_interp, ynew\n",
    "\n",
    "    x_smooth, y_smooth = smooth(x, y)\n",
    "\n",
    "    # Plot\n",
    "    fig, ax = plt.subplots(figsize=(12, 4))  # 33.35cm x 9.14cm\n",
    "    ax.set_facecolor('#ffffff')\n",
    "    fig.patch.set_facecolor('#ffffff')\n",
    "\n",
    "    ax.plot(x_smooth, y_smooth, color=color_theme, label=title, linewidth=2)\n",
    "\n",
    "    if show_dots:\n",
    "        ax.scatter(x, y, color=color_theme, s=15, zorder=3)\n",
    "\n",
    "    # Clean styling\n",
    "    for spine in ax.spines.values():\n",
    "        spine.set_visible(False)\n",
    "\n",
    "    ax.tick_params(axis='y', colors=color_theme)\n",
    "    ax.yaxis.label.set_color(color_theme)\n",
    "    ax.grid(axis='y', linestyle='-', linewidth=0.5, alpha=0.3)\n",
    "\n",
    "    ax.xaxis.set_major_formatter(DateFormatter('%d %b %Y'))\n",
    "    plt.xticks(rotation=0)\n",
    "    ax.xaxis.label.set_color('#5f6368')\n",
    "\n",
    "    # Annotate peak\n",
    "    peak_idx = np.argmax(y)\n",
    "    peak_date = x.iloc[peak_idx]\n",
    "    peak_value = y[peak_idx]\n",
    "    label = f\"{int(round(peak_value))} on {peak_date.strftime('%d %b %Y')}\"\n",
    "    ax.annotate(label, xy=(peak_date, peak_value), xytext=(peak_date, peak_value + 5),\n",
    "                ha='center', fontsize=9, color=color_theme,\n",
    "                arrowprops=dict(arrowstyle='-', color=color_theme, lw=1))\n",
    "\n",
    "    # Legend\n",
    "    ax.legend(loc='best', frameon=False)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    if save_path:\n",
    "        plt.savefig(save_path, bbox_inches='tight', dpi=150, transparent=True)\n",
    "        \n",
    "def get_data(KEYWORDS, START_DATE, END_DATE):\n",
    "    \"\"\"\n",
    "    Menghitung presence score dari data Elasticsearch menggunakan aggregation.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    KEYWORDS : List[str]\n",
    "        Daftar keyword untuk filter\n",
    "    START_DATE : str\n",
    "        Tanggal awal periode (YYYY-MM-DD)\n",
    "    END_DATE : str\n",
    "        Tanggal akhir periode (YYYY-MM-DD)\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    pd.DataFrame\n",
    "        DataFrame dengan kolom date dan presence_score\n",
    "    \"\"\"\n",
    "\n",
    "    # Definisikan semua channel dan indeks\n",
    "    default_channels = ['reddit', 'youtube', 'linkedin', 'twitter', \n",
    "                        'tiktok', 'instagram', 'facebook', 'news', 'threads']\n",
    "    indices = [f\"{ch}_data\" for ch in default_channels]\n",
    "    \n",
    "    # Build keyword filter \n",
    "    keyword_conditions = []\n",
    "    for kw in KEYWORDS:\n",
    "        keyword_conditions.append({\"match\": {\"post_caption\": {\"query\": kw, \"operator\": \"AND\"}}})\n",
    "        keyword_conditions.append({\"match\": {\"issue\": {\"query\": kw, \"operator\": \"AND\"}}})\n",
    "    \n",
    "    keyword_filter = {\n",
    "        \"bool\": {\n",
    "            \"should\": keyword_conditions,\n",
    "            \"minimum_should_match\": 1\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # Bangun query dengan date_histogram aggregation\n",
    "    # untuk mendapatkan total_mentions, total_reach, dan total_engagement per hari\n",
    "    query = {\n",
    "        \"size\": 0,\n",
    "        \"query\": {\n",
    "            \"bool\": {\n",
    "                \"must\": [\n",
    "                    {\n",
    "                        \"range\": {\n",
    "                            \"post_created_at\": {\n",
    "                                \"gte\": START_DATE,\n",
    "                                \"lte\": END_DATE\n",
    "                            }\n",
    "                        }\n",
    "                    },\n",
    "                    keyword_filter\n",
    "                ]\n",
    "            }\n",
    "        },\n",
    "        \"aggs\": {\n",
    "            \"data_per_day\": {\n",
    "                \"date_histogram\": {\n",
    "                    \"field\": \"post_created_at\",\n",
    "                    \"calendar_interval\": \"day\",\n",
    "                    \"format\": \"yyyy-MM-dd\",\n",
    "                    \"min_doc_count\": 0,\n",
    "                    \"extended_bounds\": {\n",
    "                        \"min\": START_DATE,\n",
    "                        \"max\": END_DATE\n",
    "                    }\n",
    "                },\n",
    "                \"aggs\": {\n",
    "                    \"total_reach\": {\n",
    "                        \"sum\": {\n",
    "                            \"field\": \"reach_score\"\n",
    "                        }\n",
    "                    },\n",
    "                    \"total_engagement\": {\n",
    "                        \"sum\": {\n",
    "                            \"script\": {\n",
    "                                \"source\": \"\"\"\n",
    "                                    def likes = doc.containsKey('likes') && !doc['likes'].empty ? doc['likes'].value : 0;\n",
    "                                    def shares = doc.containsKey('shares') && !doc['shares'].empty ? doc['shares'].value : 0;\n",
    "                                    def comments = doc.containsKey('comments') && !doc['comments'].empty ? doc['comments'].value : 0;\n",
    "                                    def favorites = doc.containsKey('favorites') && !doc['favorites'].empty ? doc['favorites'].value : 0;\n",
    "                                    def views = doc.containsKey('views') && !doc['views'].empty ? doc['views'].value : 0;\n",
    "                                    def retweets = doc.containsKey('retweets') && !doc['retweets'].empty ? doc['retweets'].value : 0;\n",
    "                                    def replies = doc.containsKey('replies') && !doc['replies'].empty ? doc['replies'].value : 0;\n",
    "                                    def reposts = doc.containsKey('reposts') && !doc['reposts'].empty ? doc['reposts'].value : 0;\n",
    "                                    def votes = doc.containsKey('votes') && !doc['votes'].empty ? doc['votes'].value : 0;\n",
    "                                    \n",
    "                                    return likes + shares + comments + favorites + views + retweets + replies + reposts + votes;\n",
    "                                \"\"\"\n",
    "                            }\n",
    "                        }\n",
    "                    }\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # Execute query\n",
    "    response = es.search(\n",
    "        index=\",\".join(indices),\n",
    "        body=query\n",
    "    )\n",
    "    \n",
    "    # Ekstrak data dari respons\n",
    "    buckets = response[\"aggregations\"][\"data_per_day\"][\"buckets\"]\n",
    "    \n",
    "    # Siapkan data untuk DataFrame\n",
    "    topic_data = []\n",
    "    for bucket in buckets:\n",
    "        date = bucket[\"key_as_string\"]\n",
    "        total_mentions = bucket[\"doc_count\"]\n",
    "        total_reach = bucket[\"total_reach\"][\"value\"]\n",
    "        total_engagement = bucket[\"total_engagement\"][\"value\"]\n",
    "        \n",
    "        topic_data.append({\n",
    "            \"date\": date,\n",
    "            \"total_mentions\": total_mentions,\n",
    "            \"total_reach\": total_reach,\n",
    "            \"total_engagement\": total_engagement\n",
    "        })\n",
    "    \n",
    "    # Konversi ke DataFrame\n",
    "    df_topic_data = pd.DataFrame(topic_data)\n",
    "    \n",
    "    # Jika tidak ada data, kembalikan DataFrame kosong\n",
    "    if df_topic_data.empty:\n",
    "        return pd.DataFrame(columns=[\"date\", \"presence_score\"])\n",
    "    \n",
    "    # Hitung nilai maksimum\n",
    "    max_mentions = df_topic_data[\"total_mentions\"].max()\n",
    "    max_reach = df_topic_data[\"total_reach\"].max()\n",
    "    max_engagement = df_topic_data[\"total_engagement\"].max()\n",
    "    \n",
    "    # Hitung presence_score\n",
    "    df_topic_data[\"presence_score\"] = df_topic_data.apply(\n",
    "        lambda row: round(\n",
    "            ((row[\"total_mentions\"] / max_mentions if max_mentions else 0) * 40) +\n",
    "            ((row[\"total_reach\"] / max_reach if max_reach else 0) * 40) +\n",
    "            ((row[\"total_engagement\"] / max_engagement if max_engagement else 0) * 20),\n",
    "            2\n",
    "        ),\n",
    "        axis=1\n",
    "    )\n",
    "    \n",
    "    # Pilih kolom yang diperlukan dan urutkan berdasarkan tanggal\n",
    "    result_df = df_topic_data[[\"date\", \"presence_score\"]].sort_values(by=\"date\")\n",
    "    \n",
    "    # Pastikan date dalam format string\n",
    "    result_df[\"date\"] = result_df[\"date\"].astype(str)\n",
    "    \n",
    "    return result_df\n",
    "\n",
    "def get_metrics_and_posts(FILTER_KEYWORD, high_presence_date):\n",
    "    \"\"\"\n",
    "    Mendapatkan aggregate dari sentiment per channel dan sampling data post\n",
    "    menggunakan satu query Elasticsearch untuk efisiensi.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    FILTER_KEYWORD : List[str]\n",
    "        Daftar keyword untuk filter\n",
    "    high_presence_date : str\n",
    "        Tanggal dengan presence tinggi yang akan dianalisis (YYYY-MM-DD)\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    pd.DataFrame\n",
    "        DataFrame gabungan yang berisi metrics dan posts\n",
    "    \"\"\"\n",
    "\n",
    "    \n",
    "    # Definisikan semua channel dan indeks\n",
    "    default_channels = ['reddit', 'youtube', 'linkedin', 'twitter', \n",
    "                        'tiktok', 'instagram', 'facebook', 'news', 'threads']\n",
    "    indices = [f\"{ch}_data\" for ch in default_channels]\n",
    "    \n",
    "    # Format tanggal untuk query\n",
    "    start_datetime = f\"{high_presence_date} 00:00:00\"\n",
    "    end_datetime = f\"{high_presence_date} 23:59:59\"\n",
    "    \n",
    "    # Build keyword filter \n",
    "    keyword_conditions = []\n",
    "    for kw in FILTER_KEYWORD:\n",
    "        keyword_conditions.append({\"match\": {\"post_caption\": {\"query\": kw, \"operator\": \"AND\"}}})\n",
    "        keyword_conditions.append({\"match\": {\"issue\": {\"query\": kw, \"operator\": \"AND\"}}})\n",
    "    \n",
    "    keyword_filter = {\n",
    "        \"bool\": {\n",
    "            \"should\": keyword_conditions,\n",
    "            \"minimum_should_match\": 1\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # === QUERY 1: Metrics Data ===\n",
    "    # Query untuk mendapatkan aggregate dari sentiment per channel\n",
    "    metrics_query = {\n",
    "        \"size\": 0,\n",
    "        \"query\": {\n",
    "            \"bool\": {\n",
    "                \"must\": [\n",
    "                    {\n",
    "                        \"range\": {\n",
    "                            \"post_created_at\": {\n",
    "                                \"gte\": start_datetime,\n",
    "                                \"lte\": end_datetime\n",
    "                            }\n",
    "                        }\n",
    "                    },\n",
    "                    keyword_filter\n",
    "                ]\n",
    "            }\n",
    "        },\n",
    "        \"aggs\": {\n",
    "            \"sentiment_channel\": {\n",
    "                \"terms\": {\n",
    "                    \"field\": \"sentiment\",\n",
    "                    \"size\": 10  # Jumlah sentiment teratas\n",
    "                },\n",
    "                \"aggs\": {\n",
    "                    \"channels\": {\n",
    "                        \"terms\": {\n",
    "                            \"field\": \"channel\",\n",
    "                            \"size\": 20  # Jumlah channel teratas\n",
    "                        }\n",
    "                    }\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # === QUERY 2: Posts Data ===\n",
    "    # Query untuk mendapatkan sampling data post\n",
    "    posts_query = {\n",
    "        \"size\": 100,  # Limit 100 post teratas\n",
    "        \"query\": {\n",
    "            \"bool\": {\n",
    "                \"must\": [\n",
    "                    {\n",
    "                        \"range\": {\n",
    "                            \"post_created_at\": {\n",
    "                                \"gte\": start_datetime,\n",
    "                                \"lte\": end_datetime\n",
    "                            }\n",
    "                        }\n",
    "                    },\n",
    "                    keyword_filter\n",
    "                ],\n",
    "                \"must_not\": [\n",
    "                    {\n",
    "                        \"terms\": {\n",
    "                            \"issue.keyword\": [\"not specified\", \"Not Specified\", \"NOT SPECIFIED\"]\n",
    "                        }\n",
    "                    }\n",
    "                ]\n",
    "            }\n",
    "        },\n",
    "        \"_source\": [\n",
    "            \"post_caption\", \"channel\", \"issue\", \"sentiment\", \"reach_score\", \n",
    "            \"viral_score\", \"likes\", \"shares\", \"comments\", \"favorites\", \n",
    "            \"views\", \"retweets\", \"replies\", \"reposts\", \"votes\", \"link_post\"\n",
    "        ],\n",
    "        \"sort\": [\n",
    "            {\n",
    "                \"_script\": {\n",
    "                    \"type\": \"number\",\n",
    "                    \"script\": {\n",
    "                        \"source\": \"\"\"\n",
    "                            def reach = doc.containsKey('reach_score') && !doc['reach_score'].empty ? doc['reach_score'].value : 0;\n",
    "                            def viral = doc.containsKey('viral_score') && !doc['viral_score'].empty ? doc['viral_score'].value : 0;\n",
    "                            def likes = doc.containsKey('likes') && !doc['likes'].empty ? doc['likes'].value : 0;\n",
    "                            def shares = doc.containsKey('shares') && !doc['shares'].empty ? doc['shares'].value : 0;\n",
    "                            def comments = doc.containsKey('comments') && !doc['comments'].empty ? doc['comments'].value : 0;\n",
    "                            def favorites = doc.containsKey('favorites') && !doc['favorites'].empty ? doc['favorites'].value : 0;\n",
    "                            def views = doc.containsKey('views') && !doc['views'].empty ? doc['views'].value : 0;\n",
    "                            def retweets = doc.containsKey('retweets') && !doc['retweets'].empty ? doc['retweets'].value : 0;\n",
    "                            def replies = doc.containsKey('replies') && !doc['replies'].empty ? doc['replies'].value : 0;\n",
    "                            def reposts = doc.containsKey('reposts') && !doc['reposts'].empty ? doc['reposts'].value : 0;\n",
    "                            def votes = doc.containsKey('votes') && !doc['votes'].empty ? doc['votes'].value : 0;\n",
    "                            \n",
    "                            def engagement = likes + shares + comments + favorites + views + retweets + replies + reposts + votes;\n",
    "                            \n",
    "                            return reach + viral + engagement;\n",
    "                        \"\"\"\n",
    "                    },\n",
    "                    \"order\": \"desc\"\n",
    "                }\n",
    "            }\n",
    "        ]\n",
    "    }\n",
    "    \n",
    "    # Execute kedua query secara terpisah\n",
    "    metrics_response = es.search(\n",
    "        index=\",\".join(indices),\n",
    "        body=metrics_query\n",
    "    )\n",
    "    \n",
    "    posts_response = es.search(\n",
    "        index=\",\".join(indices),\n",
    "        body=posts_query\n",
    "    )\n",
    "    \n",
    "    # Proses results untuk metrics_data\n",
    "    metrics_data = []\n",
    "    sentiment_buckets = metrics_response[\"aggregations\"][\"sentiment_channel\"][\"buckets\"]\n",
    "    \n",
    "    for sentiment_bucket in sentiment_buckets:\n",
    "        sentiment = sentiment_bucket[\"key\"]\n",
    "        channel_buckets = sentiment_bucket[\"channels\"][\"buckets\"]\n",
    "        \n",
    "        for channel_bucket in channel_buckets:\n",
    "            channel = channel_bucket[\"key\"]\n",
    "            total_mentions = channel_bucket[\"doc_count\"]\n",
    "            \n",
    "            metrics_data.append({\n",
    "                \"data_type\": \"metrics\",\n",
    "                \"sentiment\": sentiment,\n",
    "                \"channel\": channel,\n",
    "                \"total_mentions\": total_mentions,\n",
    "                \"post_caption\": None,\n",
    "                \"issue\": None,\n",
    "                \"reach_score\": None,\n",
    "                \"viral_score\": None,\n",
    "                \"engagement\": None,\n",
    "                \"link_post\": None\n",
    "            })\n",
    "    \n",
    "    # Proses results untuk posts_data\n",
    "    posts_data = []\n",
    "    post_hits = posts_response[\"hits\"][\"hits\"]\n",
    "    \n",
    "    for hit in post_hits:\n",
    "        source = hit[\"_source\"]\n",
    "        \n",
    "        # Hitung total engagement\n",
    "        engagement = sum([\n",
    "            source.get(\"likes\", 0) or 0,\n",
    "            source.get(\"shares\", 0) or 0,\n",
    "            source.get(\"comments\", 0) or 0,\n",
    "            source.get(\"favorites\", 0) or 0,\n",
    "            source.get(\"views\", 0) or 0,\n",
    "            source.get(\"retweets\", 0) or 0,\n",
    "            source.get(\"replies\", 0) or 0,\n",
    "            source.get(\"reposts\", 0) or 0,\n",
    "            source.get(\"votes\", 0) or 0\n",
    "        ])\n",
    "        \n",
    "        posts_data.append({\n",
    "            \"data_type\": \"posts\",\n",
    "            \"sentiment\": source.get(\"sentiment\", \"\"),\n",
    "            \"channel\": source.get(\"channel\", \"\"),\n",
    "            \"total_mentions\": None,\n",
    "            \"post_caption\": source.get(\"post_caption\", \"\"),\n",
    "            \"issue\": source.get(\"issue\", \"\"),\n",
    "            \"reach_score\": source.get(\"reach_score\", 0),\n",
    "            \"viral_score\": source.get(\"viral_score\", 0),\n",
    "            \"engagement\": engagement,\n",
    "            \"link_post\": source.get(\"link_post\", \"\")\n",
    "        })\n",
    "    \n",
    "    # Gabungkan kedua hasil\n",
    "    combined_data = metrics_data + posts_data\n",
    "    \n",
    "    # Konversi ke DataFrame\n",
    "    df = pd.DataFrame(combined_data)\n",
    "    \n",
    "    # Urutkan hasil\n",
    "    df = df.sort_values(\n",
    "        by=[\"data_type\", \"reach_score\", \"viral_score\", \"engagement\"],\n",
    "        ascending=[True, False, False, False],\n",
    "        na_position='last'\n",
    "    )\n",
    "    \n",
    "    return df\n",
    "\n",
    "def presence_description(TOPIC, FILTER_KEYWORD, high_presence_date,SAVE_PATH):\n",
    "    \n",
    "    combined_results = get_metrics_and_posts(\n",
    "        FILTER_KEYWORD=FILTER_KEYWORD,\n",
    "        high_presence_date=high_presence_date\n",
    "    )\n",
    "\n",
    "    # Pisahkan hasil berdasarkan data_type\n",
    "    metric_presence = combined_results[combined_results['data_type'] == 'metrics']\n",
    "    sample_post_presence = combined_results[combined_results['data_type'] == 'posts']\n",
    "\n",
    "    #summarize\n",
    "    prompt = f\"\"\"\n",
    "    You are a media analyst assistant. Analyze a spike in presence score related to the topic [{TOPIC}] on {high_presence_date}.\n",
    "\n",
    "    Here is the supporting data:\n",
    "\n",
    "    1. **Sentiment Breakdown (total mentions)**  \n",
    "    {metric_presence.groupby('sentiment').sum().to_dict()['total_mentions']}\n",
    "\n",
    "    2. **Platform Breakdown (total mentions)**  \n",
    "    {metric_presence.groupby('channel').sum().to_dict()['total_mentions']}\n",
    "\n",
    "    3. **Top Posts on {high_presence_date}**  \n",
    "    {sample_post_presence[['post_caption','channel']].to_dict(orient='records')}\n",
    "\n",
    "    ---\n",
    "\n",
    "    **Instruction:**  \n",
    "    Summarize in **1 paragraph only** what likely caused the spike, which platform contributed the most, what sentiment dominated, and what topic(s) were most discussed — **only if relevant to [{TOPIC}]**. Use a professional, concise tone.\n",
    "    \"\"\"\n",
    "    summarize = call_gemini(prompt)\n",
    "\n",
    "    with open(os.path.join(SAVE_PATH,'presence_score_analysis.json'),'w') as f:\n",
    "        json.dump({'analysis':summarize},f)\n",
    "   \n",
    "def generatre_presence_score(TOPIC, KEYWORDS, START_DATE, END_DATE, SAVE_PATH):\n",
    "    \n",
    "    presence_score = get_data(KEYWORDS, START_DATE, END_DATE)\n",
    "    \n",
    "    plot_donut_score(presence_score['presence_score'].mean(),\n",
    "                     save_path = os.path.join(SAVE_PATH,'presence_score_donut.png'))\n",
    "\n",
    "    plot_presence_score_trend(presence_score.to_dict(orient = 'records'),\n",
    "                              save_path = os.path.join(SAVE_PATH,'presence_trend.png'))\n",
    "    \n",
    "    high_presence_date = presence_score.sort_values('presence_score', ascending = False)['date'].to_list()[0]\n",
    "    \n",
    "    presence_description(TOPIC, KEYWORDS, high_presence_date, SAVE_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e23de4bd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-02T04:10:18.363346Z",
     "start_time": "2025-05-02T04:10:18.352322Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "\n",
    "KEYWORDS = ['APBN', 'Anggaran Pendapatan dan Belanja Negara', 'RAPBN', 'Kementerian Keuangan', 'Sri Mulyani', 'Defisit APBN', 'Belanja Negara', 'Pendapatan Negara', 'Pajak', 'Utang Negara', 'Subsidi', 'Transfer Daerah', 'Dana Desa', 'Postur APBN', 'APBN 2024', 'apbn']\n",
    "MAIN_TOPIC = 'APBN'\n",
    "\n",
    "start_date = '2025-04-01'\n",
    "end_date = '2025-05-01'\n",
    "START_DATE = '2025-04-01'\n",
    "END_DATE = '2025-05-01'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d232c0d0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-02T04:10:27.208839Z",
     "start_time": "2025-05-02T04:10:18.875096Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "generatre_presence_score(MAIN_TOPIC, KEYWORDS, START_DATE, END_DATE, '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d552144",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-02T07:57:36.641837Z",
     "start_time": "2025-05-02T07:57:05.116966Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from instagrapi import Client\n",
    "\n",
    "cl = Client()\n",
    "cl.login('moch_ariel', 'sukabumi03049567')\n",
    "user_id = cl.user_id_from_username('irdaniyp')\n",
    "medias = cl.user_medias(user_id, 20)  # Get the last 20 posts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d59eed44",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-02T08:00:58.161005Z",
     "start_time": "2025-05-02T08:00:58.154042Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "medias[0].caption_text"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
