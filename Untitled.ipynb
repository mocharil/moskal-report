{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a8263b0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-02T08:02:02.844598Z",
     "start_time": "2025-05-02T08:02:00.896492Z"
    }
   },
   "outputs": [],
   "source": [
    "from elasticsearch import Elasticsearch\n",
    "# Buat koneksi ke Elasticsearch\n",
    "es = Elasticsearch(\n",
    "    \"http://34.101.178.71:9200/\",\n",
    "    basic_auth=(\"elastic\", \"elasticpassword\")  # Sesuaikan dengan kredensial Anda\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fa97d64",
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Query untuk mencari dokumen dengan channel = 'news' dan date tertentu\n",
    "query_body = {\n",
    "    \"source\":[\"link_post\"],\n",
    "    \"query\": {\n",
    "        \"bool\": {\n",
    "            \"must_not\": {\n",
    "                \"exists\": {\n",
    "                    \"field\": \"viral_score\"\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "\n",
    "# Tarik data dari index 'news_data'\n",
    "response = es.search(\n",
    "    index=\"news_data\",\n",
    "    body=query_body,\n",
    "    size=10000  # Jumlah maksimal hasil yang ditarik (default hanya 10)\n",
    ")\n",
    "# Ambil hasil dokumen\n",
    "hits = response[\"hits\"][\"hits\"]\n",
    "documents = [doc[\"_source\"] for doc in hits]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a1f2802",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ed9ceb1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-02T08:11:57.315480Z",
     "start_time": "2025-05-02T08:11:57.300182Z"
    }
   },
   "outputs": [],
   "source": [
    "from datetime import datetime, timedelta\n",
    "from chart_generator.functions import *\n",
    "\n",
    "KEYWORDS = ['APBN', 'Anggaran Pendapatan dan Belanja Negara', 'RAPBN', 'Kementerian Keuangan', 'Sri Mulyani', 'Defisit APBN', 'Belanja Negara', 'Pendapatan Negara', 'Pajak', 'Utang Negara', 'Subsidi', 'Transfer Daerah', 'Dana Desa', 'Postur APBN', 'APBN 2024', 'apbn']\n",
    "MAIN_TOPIC = 'APBN'\n",
    "\n",
    "start_date = '2025-04-01'\n",
    "end_date = '2025-05-01'\n",
    "START_DATE = '2025-04-01'\n",
    "END_DATE = '2025-05-01'\n",
    "\n",
    "diff_date = range_date_count(start_date, end_date)\n",
    "prev_start_date = kurangi_tanggal(start_date, diff_date+1)\n",
    "prev_end_date = kurangi_tanggal(end_date, diff_date+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f44c21e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-02T07:47:49.743047Z",
     "start_time": "2025-05-02T07:47:49.723198Z"
    }
   },
   "outputs": [],
   "source": [
    "import re\n",
    "print('KEYWORD',KEYWORDS)\n",
    "\n",
    "FILTER_KEYWORD = []\n",
    "for key in KEYWORDS:\n",
    "    keyword = re.sub(f\"[^a-z0-9\\s]\", \" \", key.lower().strip(\" -\"))\n",
    "    FILTER_KEYWORD.append(f\"\"\"SEARCH(lower(a.post_caption), '{keyword}')\"\"\")\n",
    "FILTER_KEYWORD = '(' + ' OR '.join(FILTER_KEYWORD) + ')'\n",
    "FILTER_KEYWORD = FILTER_KEYWORD.replace('\\u2060', '')\n",
    "\n",
    "FILTER_DATE = f\"\"\"a.post_created_at BETWEEN '{start_date}' AND '{end_date}'\"\"\"\n",
    "ALL_FILTER = f\"{FILTER_DATE} AND {FILTER_KEYWORD}\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f159c56c",
   "metadata": {},
   "source": [
    "# BQ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24c6dd59",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-02T08:17:29.961910Z",
     "start_time": "2025-05-02T08:17:28.810801Z"
    },
    "code_folding": [
     6
    ]
   },
   "outputs": [],
   "source": [
    "from chart_generator.functions import *\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()  \n",
    "BQ = About_BQ(project_id= os.getenv(\"BQ_PROJECT_ID\") ,\n",
    "         credentials_loc= os.getenv(\"BQ_CREDS_LOCATION\")  )\n",
    "def join_issues_with_metrics(data, df_prediction):\n",
    "    \"\"\"\n",
    "    Join the metrics data with the prediction data based on issue mapping.\n",
    "    \n",
    "    Args:\n",
    "        data (list/DataFrame): List of dictionaries or DataFrame containing metrics data\n",
    "        df_prediction (list/DataFrame): List of dictionaries or DataFrame containing prediction data\n",
    "        \n",
    "    Returns:\n",
    "        DataFrame: Joined and aggregated DataFrame\n",
    "    \"\"\"\n",
    "    # Convert to DataFrames if they're lists\n",
    "    if isinstance(data, list):\n",
    "        data_df = pd.DataFrame(data)\n",
    "    else:\n",
    "        data_df = data.copy()\n",
    "        \n",
    "    if isinstance(df_prediction, list):\n",
    "        pred_df = pd.DataFrame(df_prediction)\n",
    "    else:\n",
    "        pred_df = df_prediction.copy()\n",
    "    \n",
    "    # Create an empty results DataFrame to store our joined data\n",
    "    results = []\n",
    "    \n",
    "    # Iterate through each unified issue\n",
    "    for _, row in pred_df.iterrows():\n",
    "        unified_issue = row['unified_issue']\n",
    "        list_issues = row['list_original_issue']\n",
    "        description = row['description']\n",
    "        \n",
    "        # Find all matching issues in data_df\n",
    "        matching_issues = data_df[data_df['issue'].isin(list_issues)]\n",
    "        \n",
    "        if len(matching_issues) > 0:\n",
    "            # Aggregate the metrics\n",
    "            total_issue = matching_issues['total_issue'].sum()\n",
    "            total_reach_score = matching_issues['total_reach_score'].sum()\n",
    "            total_viral_score = matching_issues['total_viral_score'].sum()\n",
    "            \n",
    "            # Calculate weighted averages for percentages\n",
    "            weighted_neg = np.average(\n",
    "                matching_issues['percentage_negative'], \n",
    "                weights=matching_issues['total_issue']\n",
    "            ) if len(matching_issues) > 0 else 0\n",
    "            \n",
    "            weighted_pos = np.average(\n",
    "                matching_issues['percentage_positive'], \n",
    "                weights=matching_issues['total_issue']\n",
    "            ) if len(matching_issues) > 0 else 0\n",
    "            \n",
    "            weighted_neutral = np.average(\n",
    "                matching_issues['percentage_neutral'], \n",
    "                weights=matching_issues['total_issue']\n",
    "            ) if len(matching_issues) > 0 else 0\n",
    "            \n",
    "            # Add to results\n",
    "            results.append({\n",
    "                'unified_issue': unified_issue,\n",
    "                'description': description,\n",
    "                'total_issue': total_issue,\n",
    "                'total_viral_score': total_viral_score,\n",
    "                'total_reach_score':total_reach_score,\n",
    "                'percentage_negative': round(weighted_neg, 2),\n",
    "                'percentage_positive': round(weighted_pos, 2),\n",
    "                'percentage_neutral': round(weighted_neutral, 2),\n",
    "                'list_issue': list_issues\n",
    "            })\n",
    "        else:\n",
    "            # If no matching issues were found, still include the unified issue\n",
    "            # but with zero values for metrics\n",
    "            results.append({\n",
    "                'unified_issue': unified_issue,\n",
    "                'description': description,\n",
    "                'total_issue': 0,\n",
    "                'total_viral_score': 0,\n",
    "                'total_reach_score':0,\n",
    "                'percentage_negative': 0,\n",
    "                'percentage_positive': 0,\n",
    "                'percentage_neutral': 0,\n",
    "                'list_issue': list_issues\n",
    "            })\n",
    "    \n",
    "    # Convert results to DataFrame\n",
    "    results_df = pd.DataFrame(results)\n",
    "    results_df['share_of_voice'] = results_df['total_issue']/results_df['total_issue'].sum()\n",
    "    return results_df\n",
    "\n",
    "def get_references(rf):\n",
    "    # Hitung jumlah unique channel\n",
    "    unique_channels = rf['channel'].nunique()\n",
    "\n",
    "    # Tentukan jumlah top posts per channel berdasarkan jumlah unique channel\n",
    "    if unique_channels <= 2:\n",
    "        # Jika channel <= 2, ambil top 3 untuk setiap channel\n",
    "        n_per_channel = 3\n",
    "    elif unique_channels == 3:\n",
    "        # Jika channel = 3, ambil top 2 untuk setiap channel\n",
    "        n_per_channel = 2\n",
    "    else:\n",
    "        # Jika channel >= 4, ambil top 1 untuk setiap channel\n",
    "        n_per_channel = 1\n",
    "        # Batasi pada 6 channel teratas jika terlalu banyak channel\n",
    "        if unique_channels > 6:\n",
    "            # Dapatkan 6 channel dengan rata-rata reach_score tertinggi\n",
    "            top_channels = rf.groupby('channel')['reach_score'].mean().nlargest(6).index.tolist()\n",
    "            # Filter rf hanya untuk channel-channel tersebut\n",
    "            rf_filtered = rf[rf['channel'].isin(top_channels)]\n",
    "        else:\n",
    "            rf_filtered = rf\n",
    "\n",
    "    # Ambil top n_per_channel posts untuk setiap channel\n",
    "    if unique_channels > 6:\n",
    "        # Jika sudah difilter ke 6 channel teratas\n",
    "        top_posts = rf_filtered.groupby('channel').apply(\n",
    "            lambda x: x.nlargest(n_per_channel, 'reach_score')\n",
    "        ).reset_index(drop=True)\n",
    "    else:\n",
    "        # Jika jumlah channel <= 6\n",
    "        top_posts = rf.groupby('channel').apply(\n",
    "            lambda x: x.nlargest(n_per_channel, 'reach_score')\n",
    "        ).reset_index(drop=True)\n",
    "\n",
    "    # Tampilkan hasil\n",
    "    print(f\"Total posts: {len(top_posts)}\")\n",
    "    return (top_posts[['channel', 'link_post']])\n",
    "\n",
    "def generate_topic_overview(ALL_FILTER, TOPIC, SAVE_PATH = 'PPT'):\n",
    "    # GET DATA\n",
    "    query = f\"\"\"\n",
    "        WITH filtered_posts AS (\n",
    "            SELECT\n",
    "                c.issue,\n",
    "                a.viral_score,\n",
    "                c.sentiment,\n",
    "                a.post_caption,\n",
    "                a.reach_score\n",
    "            FROM medsos.post_analysis a\n",
    "            JOIN medsos.post_category c\n",
    "                ON a.link_post = c.link_post\n",
    "            WHERE\n",
    "                {ALL_FILTER}\n",
    "                AND lower(issue) NOT IN ('not specified')\n",
    "        )\n",
    "\n",
    "        SELECT\n",
    "            issue,\n",
    "            COUNT(issue) AS total_issue,\n",
    "            SUM(viral_score) AS total_viral_score,\n",
    "            SUM(reach_score) AS total_reach_score,\n",
    "            ROUND(100 * SUM(CASE WHEN sentiment = 'negative' THEN 1 ELSE 0 END) / COUNT(*), 2) AS percentage_negative,\n",
    "            ROUND(100 * SUM(CASE WHEN sentiment = 'positive' THEN 1 ELSE 0 END) / COUNT(*), 2) AS percentage_positive,\n",
    "            ROUND(100 * SUM(CASE WHEN sentiment = 'neutral' THEN 1 ELSE 0 END) / COUNT(*), 2) AS percentage_neutral,\n",
    "            ARRAY_AGG(post_caption ORDER BY viral_score DESC LIMIT 3) AS top_post_captions\n",
    "        FROM filtered_posts\n",
    "        GROUP BY issue\n",
    "        ORDER BY total_issue DESC, total_viral_score DESC\n",
    "        LIMIT 40;\"\"\"\n",
    "    data = BQ.to_pull_data(query)\n",
    "\n",
    "    # Summarize Topics\n",
    "    prompt = f\"\"\"\n",
    "    You are a Social Media Analyst Expert. This is the topic about [{TOPIC}]\n",
    "    Your task is to analyze and group similar issues together based on their meaning, then generate a concise and meaningful description based on the provided post captions.\n",
    "\n",
    "    ### Instructions:\n",
    "    1. Identify and **group similar issues** under a single **unified issue name** that best represents the grouped topics.\n",
    "    2. For each unified issue:\n",
    "       - Create a **list_original_issue** containing ONLY the unique original issue names grouped under this unified issue.\n",
    "       - IMPORTANT: After collecting all original issues, REMOVE ALL DUPLICATES before finalizing the list_original_issue.\n",
    "       - Process each string to ensure exact duplicates (including case and spacing) are removed.\n",
    "       - Check for and eliminate semantic duplicates (same meaning but slightly different wording).\n",
    "    3. Use the **top_post_captions** from each issue group to generate a **short and insightful description** summarizing the key discussions.\n",
    "    4. Remove from analysis if you found that the post is not related to the Topic\n",
    "    5. Format the output as **valid JSON**.\n",
    "\n",
    "    ### Data:\n",
    "    {data[['issue', 'top_post_captions']].to_dict(orient='records')}\n",
    "\n",
    "    ### Output Format (JSON):\n",
    "    [\n",
    "      {{\n",
    "        \"unified_issue\": \"<Unified issue name>\",\n",
    "        \"list_original_issue\": [\"<issue 1>\", \"<issue 2>\", \"<issue 3>\"],\n",
    "        \"description\": \"<A short yet meaningful analysis considering key discussion points from the captions, jangan gunakan tanda kutip \\\" tapi gunakan \\`>\"\n",
    "      }}\n",
    "    ]\n",
    "\n",
    "    ### Critical Rules for list_original_issue:\n",
    "    - Before finalizing each list_original_issue, PERFORM AN EXPLICIT DEDUPLICATION check.\n",
    "    - VERIFY that NO DUPLICATE ENTRIES appear in any list_original_issue.\n",
    "    - First remove exact duplicates, then check for semantic duplicates (like \"Student protests against government\" vs \"Student protests against government policies\").\n",
    "    - Each original issue must appear in EXACTLY ONE unified group.\n",
    "    - Double-check the final output to ensure no duplicates remain in any list_original_issue.\n",
    "    \"\"\"\n",
    "\n",
    "    \n",
    "    prediction = call_gemini(prompt)\n",
    "    df_prediction = pd.DataFrame(eval(re.findall(r'\\[.*\\]',prediction, flags=re.I|re.S)[0]))\n",
    "\n",
    "    result_df = join_issues_with_metrics(data, df_prediction)\n",
    "\n",
    "    # Get References\n",
    "    query = f\"\"\"\n",
    "    select c.link_post, c.issue,  reach_score, a.channel from medsos.post_category  c\n",
    "    JOIN medsos.post_analysis a\n",
    "    ON a.link_post = c.link_post\n",
    "    where c.issue in {tuple([j for i in result_df.head(5)['list_issue'] for j in i])}\n",
    "    order by viral_score desc \n",
    "\n",
    "    \"\"\"\n",
    "    list_berita = BQ.to_pull_data(query)\n",
    "    #reach_score,viral_score, influence_score,\n",
    "    references = []\n",
    "    for issues in result_df['list_issue']:\n",
    "        rf = list_berita[list_berita['issue'].isin(issues)]\n",
    "        channels = rf['channel'].nunique()\n",
    "        references.append(get_references(rf).to_dict(orient = 'records'))\n",
    "\n",
    "    result_df['references'] = references\n",
    "\n",
    "    with open(os.path.join(SAVE_PATH,'topic_overview.json'),'w') as f:\n",
    "        for i in result_df.to_dict(orient = 'records'):\n",
    "            f.write(json.dumps(i)+'\\n')\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "442ce4cf",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-02T08:17:36.823382Z",
     "start_time": "2025-05-02T08:17:31.046795Z"
    },
    "code_folding": [
     4
    ],
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from datetime import datetime, timedelta\n",
    "from chart_generator.functions import *\n",
    "\n",
    "KEYWORDS = ['APBN', 'Anggaran Pendapatan dan Belanja Negara', 'RAPBN', 'Kementerian Keuangan', 'Sri Mulyani', 'Defisit APBN', 'Belanja Negara', 'Pendapatan Negara', 'Pajak', 'Utang Negara', 'Subsidi', 'Transfer Daerah', 'Dana Desa', 'Postur APBN', 'APBN 2024', 'apbn']\n",
    "MAIN_TOPIC = 'APBN'\n",
    "\n",
    "start_date = '2025-04-01'\n",
    "end_date = '2025-05-01'\n",
    "START_DATE = '2025-04-01'\n",
    "END_DATE = '2025-05-01'\n",
    "\n",
    "diff_date = range_date_count(start_date, end_date)\n",
    "prev_start_date = kurangi_tanggal(start_date, diff_date+1)\n",
    "prev_end_date = kurangi_tanggal(end_date, diff_date+1)\n",
    "\n",
    "generate_sentiment_analysis(MAIN_TOPIC, KEYWORDS,START_DATE,END_DATE, '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6818ca1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-02T08:10:12.642561Z",
     "start_time": "2025-05-02T08:10:12.344332Z"
    },
    "code_folding": [
     4
    ]
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5018f148",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-02T08:08:11.756739Z",
     "start_time": "2025-05-02T08:08:07.733173Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "sentiment_counts, pivot_sentiment = get_sentiment_data(\n",
    "    KEYWORDS=KEYWORDS,\n",
    "    START_DATE=START_DATE,\n",
    "    END_DATE=END_DATE\n",
    ")\n",
    "#breakdown\n",
    "plot_half_donut_sentiment(sentiment_counts, title='Sentiment Distribution',\n",
    "                      save_path = os.path.join(SAVE_PATH,'sentiment_breakdown.png'))\n",
    "\n",
    "#per platform\n",
    "plot_sentiment_by_channel(pivot_sentiment, save_path = os.path.join(SAVE_PATH,'sentiment_by_categories.png'))\n",
    "\n",
    "#summarize\n",
    "summarize(TOPIC,ALL_FILTER,sentiment_counts,pivot_sentiment, SAVE_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4d284834",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-02T11:31:30.836666Z",
     "start_time": "2025-05-02T11:31:27.208865Z"
    }
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "from utils.list_of_mentions import get_filtered_mentions\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def generate_topic_overview(TOPIC, KEYWORDS, START_DATE, END_DATE, SAVE_PATH):\n",
    "    url = \"https://moskal-api-v2-924335637775.us-central1.run.app/api/v2/topics-overview\"\n",
    "\n",
    "    headers = {\n",
    "        \"accept\": \"application/json\",\n",
    "        \"Content-Type\": \"application/json\"\n",
    "    }\n",
    "\n",
    "    payload = {\n",
    "        \"keywords\": KEYWORDS,\n",
    "        \"sentiment\": [\"positive\", \"negative\", \"neutral\"],\n",
    "        \"date_filter\": \"custom\",\n",
    "        \"custom_start_date\": START_DATE,\n",
    "        \"custom_end_date\": END_DATE,\n",
    "        \"owner_id\": \"5\",\n",
    "        \"project_name\": TOPIC\n",
    "    }\n",
    "\n",
    "    response = requests.post(url, headers=headers, json=payload)\n",
    "\n",
    "    df_topics = pd.DataFrame(response.json())\n",
    "\n",
    "    df_topics = df_topics.rename(columns = {'viral_score':'total_viral_score',\n",
    "                            'reach_score':'total_reach_score',\n",
    "                            'total_posts':'total_issue'})\n",
    "\n",
    "    df_topics['percentage_negative'] = df_topics['negative']/df_topics['total_issue']*100\n",
    "    df_topics['percentage_positive'] = df_topics['positive']/df_topics['total_issue']*100\n",
    "    df_topics['percentage_neutral'] = df_topics['neutral']/df_topics['total_issue']*100\n",
    "\n",
    "\n",
    "    df_top_final = df_topics[:5]\n",
    "    list_references = []\n",
    "    for list_issue in df_top_final['list_issue']:\n",
    "        references = get_filtered_mentions(\n",
    "            keywords=list_issue,\n",
    "            start_date=START_DATE,\n",
    "            end_date=END_DATE,\n",
    "            source=[\"channel\",'link_post'],\n",
    "            sort_type='popular',\n",
    "            page=1,\n",
    "            page_size=5\n",
    "        ) \n",
    "        list_references.append(references['data'])\n",
    "    df_top_final['references'] = list_references\n",
    "\n",
    "    with open(os.path.join(SAVE_PATH,'topic_overview.json'),'w') as f:\n",
    "        for i in df_top_final.to_dict(orient = 'records'):\n",
    "            f.write(json.dumps(i)+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "28533436",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-02T11:31:52.014036Z",
     "start_time": "2025-05-02T11:31:37.797417Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "V2\n",
      "-------------->GEMINI_CREDS_LOCATION: skilled-compass.json\n",
      "-------------->GEMINI_PROJECT_ID: paper-ds-production\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime, timedelta\n",
    "from chart_generator.functions import *\n",
    "\n",
    "KEYWORDS = ['APBN', 'Anggaran Pendapatan dan Belanja Negara', 'RAPBN', 'Kementerian Keuangan', 'Sri Mulyani', 'Defisit APBN', 'Belanja Negara', 'Pendapatan Negara', 'Pajak', 'Utang Negara', 'Subsidi', 'Transfer Daerah', 'Dana Desa', 'Postur APBN', 'APBN 2024', 'apbn']\n",
    "MAIN_TOPIC = 'APBN'\n",
    "\n",
    "start_date = '2025-04-01'\n",
    "end_date = '2025-05-01'\n",
    "START_DATE = '2025-04-01'\n",
    "END_DATE = '2025-05-01'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f7290d4e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-02T11:32:09.897940Z",
     "start_time": "2025-05-02T11:31:57.358680Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Aril Indra Permana\\AppData\\Local\\Temp\\ipykernel_68900\\685608068.py:50: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_top_final['references'] = list_references\n"
     ]
    }
   ],
   "source": [
    "generate_topic_overview(MAIN_TOPIC, KEYWORDS, START_DATE, END_DATE, '')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28a83509",
   "metadata": {},
   "source": [
    "# REKOMENDASI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "189d63ae",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-02T11:54:10.563646Z",
     "start_time": "2025-05-02T11:54:05.716189Z"
    },
    "code_folding": [
     9
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded sentiment breakdown data\n",
      "Loaded sentiment by categories data\n",
      "Loaded presence score analysis\n",
      "Loaded sentiment analysis\n",
      "Loaded popular mentions data\n",
      "Loaded KOL data\n",
      "Loaded topic overview data\n",
      "Prompt generated, calling LLM...\n",
      "Direct JSON parsing failed, trying regex extraction...\n",
      "Recommendations generated and saved to REPORT\\ruu tni\\2025-04-02 - 2025-05-02\\recommendations.json\n",
      "[\n",
      "  {\n",
      "    \"title\": \"Address Negative Sentiment on TNI Power Expansion\",\n",
      "    \"actions\": [\n",
      "      \"Develop a series of explainer videos and infographics for news platforms clarifying the specific limitations and oversight mechanisms within the RUU TNI regarding civilian affairs, directly addressing concerns about a return to authoritarianism.\",\n",
      "      \"Organize a virtual town hall meeting with TNI representatives, legal experts, and civil society organizations, broadcast on YouTube and news websites, to openly discuss and answer public questions about the scope of TNI authority under the revised law.\",\n",
      "      \"Partner with trusted news outlets to publish articles and op-eds featuring diverse perspectives on the RUU TNI, including voices from human rights organizations and legal scholars, to foster a balanced and informed public discourse.\"\n",
      "    ]\n",
      "  },\n",
      "  {\n",
      "    \"title\": \"Boost Positive Narratives on TNI's National Role\",\n",
      "    \"actions\": [\n",
      "      \"Create a social media campaign showcasing the TNI's positive contributions to national security, disaster relief, and humanitarian efforts, using compelling visuals and personal stories to highlight their dedication and professionalism.\",\n",
      "      \"Collaborate with Instagram influencers and TikTok creators to produce engaging content that demonstrates the TNI's commitment to serving the people and protecting the nation, emphasizing their role in maintaining stability and security.\",\n",
      "      \"Develop a series of short documentaries highlighting the TNI's involvement in community development projects and their collaboration with civilian organizations, showcasing their positive impact on local communities and fostering trust.\"\n",
      "    ]\n",
      "  },\n",
      "  {\n",
      "    \"title\": \"Counter Misinformation on Land Disputes\",\n",
      "    \"actions\": [\n",
      "      \"Establish a dedicated communication channel on the TNI website and social media platforms to provide accurate and timely information about land disputes involving the TNI, addressing misinformation and clarifying the TNI's position.\",\n",
      "      \"Proactively engage with news outlets and social media users to correct inaccurate reporting and address concerns about land disputes, ensuring that the TNI's perspective is accurately represented and that affected communities are heard.\",\n",
      "      \"Develop a transparent and accessible process for resolving land disputes involving the TNI, including mediation and legal recourse, and communicate this process clearly to the public to build trust and accountability.\"\n",
      "    ]\n",
      "  },\n",
      "  {\n",
      "    \"title\": \"Engage Key Opinion Leaders for Balanced Dialogue\",\n",
      "    \"actions\": [\n",
      "      \"Invite key opinion leaders, including those with critical perspectives, to participate in closed-door briefings and discussions about the RUU TNI, providing them with accurate information and addressing their concerns directly.\",\n",
      "      \"Partner with neutral and respected influencers on TikTok and Twitter to create content that promotes informed discussion and critical thinking about the RUU TNI, encouraging followers to engage in respectful dialogue and avoid spreading misinformation.\",\n",
      "      \"Monitor social media conversations and identify emerging narratives and concerns, using this information to tailor communication strategies and address specific issues raised by key opinion leaders and the public.\"\n",
      "    ]\n",
      "  }\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "import pandas as pd\n",
    "import re\n",
    "from typing import Dict, List, Any, Union\n",
    "\n",
    "# Import utilitas dari paket utils (sesuaikan dengan environment Anda)\n",
    "from chart_generator.functions import call_gemini\n",
    "\n",
    "def generate_recommendations(TOPIC,SAVE_PATH):\n",
    "   \n",
    "    # Inisialisasi dictionary untuk menyimpan semua data\n",
    "    data = {}\n",
    "    \n",
    "    # 1. Load sentiment_breakdown.json\n",
    "    try:\n",
    "        with open(os.path.join(SAVE_PATH, 'sentiment_breakdown.json'), 'r') as f:\n",
    "            data['sentiment_counts'] = json.load(f)\n",
    "        print(\"Loaded sentiment breakdown data\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading sentiment breakdown: {e}\")\n",
    "        data['sentiment_counts'] = {}\n",
    "    \n",
    "    # 2. Load sentiment_by_categories.json\n",
    "    try:\n",
    "        with open(os.path.join(SAVE_PATH, 'sentiment_by_categories.json'), 'r') as f:\n",
    "            data['pivot_sentiment'] = json.load(f)\n",
    "        print(\"Loaded sentiment by categories data\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading sentiment by categories: {e}\")\n",
    "        data['pivot_sentiment'] = []\n",
    "    \n",
    "    # 3. Load presence_score_analysis.json\n",
    "    try:\n",
    "        with open(os.path.join(SAVE_PATH, 'presence_score_analysis.json'), 'r') as f:\n",
    "            data['presence_score_analysis'] = json.load(f)\n",
    "        print(\"Loaded presence score analysis\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading presence score analysis: {e}\")\n",
    "        data['presence_score_analysis'] = {}\n",
    "    \n",
    "    # 4. Load sentiment_analysis.json\n",
    "    try:\n",
    "        with open(os.path.join(SAVE_PATH, 'sentiment_analysis.json'), 'r') as f:\n",
    "            data['sentiment_analysis'] = json.load(f)\n",
    "        print(\"Loaded sentiment analysis\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading sentiment analysis: {e}\")\n",
    "        data['sentiment_analysis'] = {}\n",
    "    \n",
    "    # 5. Load popular_mentions.csv\n",
    "    try:\n",
    "        data['popular_mentions'] = pd.read_csv(os.path.join(SAVE_PATH, 'popular_mentions.csv')).to_dict(orient='records')[:10]  # Limit to top 10\n",
    "        print(\"Loaded popular mentions data\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading popular mentions: {e}\")\n",
    "        data['popular_mentions'] = []\n",
    "    \n",
    "    # 6. Load KOL data\n",
    "    try:\n",
    "        kol_data = []\n",
    "        with open(os.path.join(SAVE_PATH, 'kol.json'), 'r') as f:\n",
    "            for line in f:\n",
    "                kol_data.append(json.loads(line.strip()))\n",
    "        data['kol_data'] = kol_data[:10]  # Limit to top 10\n",
    "        print(\"Loaded KOL data\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading KOL data: {e}\")\n",
    "        data['kol_data'] = []\n",
    "    \n",
    "    # 7. Load topic_overview.json\n",
    "    try:\n",
    "        topic_data = []\n",
    "        with open(os.path.join(SAVE_PATH, 'topic_overview.json'), 'r') as f:\n",
    "            for line in f:\n",
    "                topic_data.append(json.loads(line.strip()))\n",
    "        data['top_entities'] = topic_data[:15]  # Limit to top 15\n",
    "        print(\"Loaded topic overview data\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading topic overview: {e}\")\n",
    "        data['top_entities'] = []\n",
    "    \n",
    "    # Extract platform data from sentiment_by_categories.json for list of top platforms\n",
    "    top_platforms = []\n",
    "    if data['pivot_sentiment']:\n",
    "        # Sort by total_mentions and get top 5 platforms\n",
    "        sorted_platforms = sorted(data['pivot_sentiment'], key=lambda x: x.get('total_mentions', 0), reverse=True)\n",
    "        top_platforms = [p.get('channel', '') for p in sorted_platforms[:5] if p.get('channel')]\n",
    "    \n",
    "    # Build the prompt\n",
    "    prompt = f\"\"\"\n",
    "    You are a senior digital communications and media strategy expert. Analyze the following comprehensive data about [{TOPIC}] for the period [{START_DATE}] to [{END_DATE}] and provide strategic recommendations.\n",
    "\n",
    "    ## SENTIMENT DATA\n",
    "    \n",
    "    Overall sentiment distribution:\n",
    "    {data['sentiment_counts']}\n",
    "    \n",
    "    Sentiment by platform/channel:\n",
    "    {data['pivot_sentiment'][:5] if data['pivot_sentiment'] else \"No data available\"}\n",
    "    \"\"\"\n",
    "    \n",
    "    # Add presence score analysis if available\n",
    "    if data['presence_score_analysis']:\n",
    "        prompt += f\"\"\"\n",
    "    ## PRESENCE SCORE ANALYSIS\n",
    "    \n",
    "    Overall presence score trends:\n",
    "    {data['presence_score_analysis']}\n",
    "    \"\"\"\n",
    "    \n",
    "    # Add sentiment analysis if available\n",
    "    if data['sentiment_analysis']:\n",
    "        prompt += f\"\"\"\n",
    "    ## DETAILED SENTIMENT ANALYSIS\n",
    "    \n",
    "    In-depth sentiment analysis:\n",
    "    {data['sentiment_analysis']}\n",
    "    \"\"\"\n",
    "    \n",
    "    # Add popular mentions if available\n",
    "    if data['popular_mentions']:\n",
    "        prompt += f\"\"\"\n",
    "    ## POPULAR MENTIONS\n",
    "    \n",
    "    Sample of popular posts (top shown):\n",
    "    {data['popular_mentions'][:3]}\n",
    "    \"\"\"\n",
    "    \n",
    "    # Add top entities if available\n",
    "    if data['top_entities']:\n",
    "        prompt += f\"\"\"\n",
    "    ## TOP ENTITIES/TOPICS\n",
    "    \n",
    "    Most discussed entities or subtopics:\n",
    "    {data['top_entities'][:10]}\n",
    "    \"\"\"\n",
    "    \n",
    "    # Add KOL data if available\n",
    "    if data['kol_data']:\n",
    "        prompt += f\"\"\"\n",
    "    ## KEY OPINION LEADERS\n",
    "    \n",
    "    Top influencers discussing this topic:\n",
    "    {data['kol_data'][:5]}\n",
    "    \"\"\"\n",
    "    \n",
    "    # Add list of top platforms\n",
    "    prompt += f\"\"\"\n",
    "    ## TOP PLATFORMS\n",
    "    \n",
    "    The most active platforms for this topic are:\n",
    "    {top_platforms if top_platforms else \"Data not available\"}\n",
    "    \"\"\"\n",
    "    \n",
    "    # Instruksi dan format output\n",
    "    prompt += f\"\"\"\n",
    "    ## TASK\n",
    "    \n",
    "    Based on all the data above, provide strategic communication recommendations for managing and improving the discourse around [{TOPIC}].\n",
    "\n",
    "    Format your response as a JSON array with EXACTLY 4 recommendation categories (no more, no less), each with:\n",
    "    - \"title\": A clear, specific category description focusing on communication strategy (limit to 10 words)\n",
    "    - \"actions\": An array of EXACTLY 3 specific, actionable steps (each 1-2 sentences long)\n",
    "\n",
    "    The recommendations should be highly specific to {TOPIC} and directly address the sentiment, trends, and key concerns found in the data. DO NOT provide generic advice.\n",
    "\n",
    "    Each recommendation should be concrete, implementable, and directly tied to the data insights provided.\n",
    "\n",
    "    THE OUTPUT MUST BE VALID JSON. Use this exact format:\n",
    "    [\n",
    "      {{\n",
    "        \"title\": \"Strategy Category Title\",\n",
    "        \"actions\": [\n",
    "          \"Specific actionable step one with clear direction.\",\n",
    "          \"Specific actionable step two with clear direction.\",\n",
    "          \"Specific actionable step three with clear direction.\"\n",
    "        ]\n",
    "      }},\n",
    "      ...\n",
    "    ]\n",
    "\n",
    "    IMPORTANT: Ensure your output is ONLY the properly formatted JSON with no additional text, markdown, or explanatory content. The JSON array must contain EXACTLY 4 recommendation objects. Each recommendation must have EXACTLY 3 actions.\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"Prompt generated, calling LLM...\")\n",
    "    \n",
    "    try:\n",
    "        # Call LLM to generate recommendations\n",
    "        recommendations_text = call_gemini(prompt)\n",
    "        \n",
    "        # Try to parse JSON directly\n",
    "        try:\n",
    "            recommendations = json.loads(recommendations_text)\n",
    "        except json.JSONDecodeError:\n",
    "            # Use regex as fallback if direct parsing fails\n",
    "            print(\"Direct JSON parsing failed, trying regex extraction...\")\n",
    "            json_pattern = re.compile(r'\\[\\s*\\{.*\\}\\s*\\]', re.DOTALL)\n",
    "            json_match = json_pattern.search(recommendations_text)\n",
    "            \n",
    "            if json_match:\n",
    "                json_str = json_match.group(0)\n",
    "                recommendations = json.loads(json_str)\n",
    "            else:\n",
    "                # Last-resort fallback parsing for valid portions\n",
    "                print(\"Regex extraction failed, trying fallback parsing...\")\n",
    "                title_pattern = re.compile(r'\"title\":\\s*\"([^\"]+)\"')\n",
    "                actions_pattern = re.compile(r'\"actions\":\\s*\\[(.*?)\\]', re.DOTALL)\n",
    "                action_item_pattern = re.compile(r'\"([^\"]+)\"')\n",
    "                \n",
    "                recommendations = []\n",
    "                \n",
    "                # Find all title matches\n",
    "                title_matches = title_pattern.finditer(recommendations_text)\n",
    "                for title_match in title_matches:\n",
    "                    title = title_match.group(1)\n",
    "                    # Find corresponding actions\n",
    "                    actions_match = actions_pattern.search(recommendations_text, title_match.end())\n",
    "                    if actions_match:\n",
    "                        actions_text = actions_match.group(1)\n",
    "                        actions = [m.group(1) for m in action_item_pattern.finditer(actions_text)]\n",
    "                        if actions:\n",
    "                            recommendations.append({\"title\": title, \"actions\": actions})\n",
    "                \n",
    "        # Save the recommendations to file\n",
    "        with open(os.path.join(SAVE_PATH, 'recommendations.json'), 'w') as f:\n",
    "            json.dump(recommendations, f, indent=2)\n",
    "            \n",
    "        print(f\"Recommendations generated and saved to {os.path.join(SAVE_PATH, 'recommendations.json')}\")\n",
    "        return recommendations\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error generating recommendations: {e}\")\n",
    "        # Return fallback recommendations\n",
    "        fallback_recommendations = [\n",
    "            {\n",
    "                \"title\": \"Error Processing Data\",\n",
    "                \"actions\": [\n",
    "                    \"Review input data files for completeness and formatting issues.\",\n",
    "                    \"Check LLM connection and retry with simplified prompt.\",\n",
    "                    \"Consider manual analysis if automation continues to fail.\"\n",
    "                ]\n",
    "            }\n",
    "        ]\n",
    "        \n",
    "        with open(os.path.join(SAVE_PATH, 'recommendations_error.json'), 'w') as f:\n",
    "            json.dump(fallback_recommendations, f, indent=2)\n",
    "            \n",
    "        return fallback_recommendations\n",
    "\n",
    "\n",
    "SAVE_PATH = os.path.join('REPORT', 'ruu tni', \"2025-04-02 - 2025-05-02\")\n",
    "recommendations = generate_strategic_recommendations(\n",
    "    'RUU TNI',\n",
    "    SAVE_PATH\n",
    ")\n",
    "\n",
    "print(json.dumps(recommendations, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "49d1de2d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-02T12:06:30.054243Z",
     "start_time": "2025-05-02T12:06:23.613779Z"
    },
    "code_folding": [
     212
    ],
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded sentiment breakdown data\n",
      "Loaded sentiment by categories data\n",
      "Loaded presence score analysis\n",
      "Loaded sentiment analysis\n",
      "Loaded popular mentions data\n",
      "Loaded KOL data\n",
      "Loaded topic overview data\n",
      "Loaded recommendations data\n",
      "Prompt generated, calling LLM...\n",
      "Executive summary generated and saved to REPORT\\ruu tni\\2025-04-02 - 2025-05-02\\executive_summary.json\n",
      "{\n",
      "  \"summary\": {\n",
      "    \"scope_and_sentiment\": {\n",
      "      \"title\": \"\\ud83d\\udd39 Scope and Overall Sentiment:\",\n",
      "      \"points\": [\n",
      "        \"The RUU TNI garnered 1.2 million mentions across social media platforms between April 1st and May 1st, 2025. The dominant sentiment was neutral (55%), indicating a largely informational or observational tone.\",\n",
      "        \"Negative mentions constituted 28% of the total, suggesting significant concerns or criticisms surrounding specific aspects of the proposed legislation.\",\n",
      "        \"The coverage was a mix of factual reporting (40%), opinion-based commentary (45%), and emotionally charged reactions (15%).\"\n",
      "      ]\n",
      "    },\n",
      "    \"dominant_topics\": {\n",
      "      \"title\": \"\\ud83d\\udd39 Dominant Topics:\",\n",
      "      \"topics\": [\n",
      "        {\n",
      "          \"name\": \"Scope of Military Authority\",\n",
      "          \"reach\": \"Approximately 450,000 mentions\",\n",
      "          \"sentiment\": \"20% Positive, 40% Neutral, 40% Negative\",\n",
      "          \"key_points\": [\n",
      "            \"Concerns were raised about potential overreach and impact on civilian governance.\",\n",
      "            \"Positive sentiment stemmed from perceived need for stronger national security measures.\"\n",
      "          ]\n",
      "        },\n",
      "        {\n",
      "          \"name\": \"Accountability and Oversight Mechanisms\",\n",
      "          \"reach\": \"Approximately 300,000 mentions\",\n",
      "          \"sentiment\": \"15% Positive, 50% Neutral, 35% Negative\",\n",
      "          \"key_points\": [\n",
      "            \"Demands for robust independent oversight to prevent abuse of power were prevalent.\",\n",
      "            \"Positive sentiment focused on proposed improvements to existing accountability frameworks.\"\n",
      "          ]\n",
      "        },\n",
      "        {\n",
      "          \"name\": \"Impact on Civil Liberties\",\n",
      "          \"reach\": \"Approximately 250,000 mentions\",\n",
      "          \"sentiment\": \"10% Positive, 45% Neutral, 45% Negative\",\n",
      "          \"key_points\": [\n",
      "            \"Fears were expressed regarding potential restrictions on freedom of speech and assembly.\",\n",
      "            \"Positive sentiment highlighted potential benefits for national stability.\"\n",
      "          ]\n",
      "        },\n",
      "        {\n",
      "          \"name\": \"Budget Allocation for TNI Modernization\",\n",
      "          \"reach\": \"Approximately 100,000 mentions\",\n",
      "          \"sentiment\": \"30% Positive, 50% Neutral, 20% Negative\",\n",
      "          \"key_points\": [\n",
      "            \"Support for increased funding to enhance military capabilities was evident.\",\n",
      "            \"Negative sentiment focused on concerns about transparency and efficient resource management.\"\n",
      "          ]\n",
      "        },\n",
      "        {\n",
      "          \"name\": \"Role of TNI in Internal Security\",\n",
      "          \"reach\": \"Approximately 80,000 mentions\",\n",
      "          \"sentiment\": \"25% Positive, 55% Neutral, 20% Negative\",\n",
      "          \"key_points\": [\n",
      "            \"Debate centered on the appropriate balance between military involvement and civilian law enforcement.\",\n",
      "            \"Positive sentiment emphasized the need for TNI assistance in combating terrorism and organized crime.\"\n",
      "          ]\n",
      "        }\n",
      "      ]\n",
      "    },\n",
      "    \"peak_periods\": {\n",
      "      \"title\": \"\\ud83d\\udd39 Peak Periods:\",\n",
      "      \"points\": [\n",
      "        \"April 10th - April 15th, 2025: Highest positive coverage following a government announcement detailing proposed amendments to address public concerns.\",\n",
      "        \"April 20th - April 25th, 2025: Highest engagement driven by a viral video featuring a prominent activist criticizing the RUU TNI.\"\n",
      "      ]\n",
      "    },\n",
      "    \"negative_sentiment\": {\n",
      "      \"title\": \"\\ud83d\\udd39 Negative Sentiment Analysis:\",\n",
      "      \"mentions\": [\n",
      "        {\n",
      "          \"source\": \"Twitter\",\n",
      "          \"description\": \"Concerns about potential erosion of civilian oversight and increased military influence in politics.\"\n",
      "        },\n",
      "        {\n",
      "          \"source\": \"Facebook\",\n",
      "          \"description\": \"Criticism of perceived lack of transparency in the drafting process and limited public consultation.\"\n",
      "        },\n",
      "        {\n",
      "          \"source\": \"Online News Portals\",\n",
      "          \"description\": \"Reports highlighting potential human rights implications and risks of abuse of power.\"\n",
      "        },\n",
      "        {\n",
      "          \"source\": \"Instagram\",\n",
      "          \"description\": \"Visual memes and infographics expressing anxieties about restrictions on civil liberties.\"\n",
      "        }\n",
      "      ]\n",
      "    },\n",
      "    \"key_recommendations\": {\n",
      "      \"title\": \"\\ud83d\\udd39 Key Recommendations:\",\n",
      "      \"points\": [\n",
      "        \"Prioritize transparency and public engagement by actively addressing concerns and providing clear explanations of the RUU TNI's provisions.\",\n",
      "        \"Strengthen accountability mechanisms and independent oversight bodies to ensure responsible exercise of military authority.\",\n",
      "        \"Address concerns about civil liberties by incorporating safeguards to protect freedom of speech, assembly, and other fundamental rights.\"\n",
      "      ]\n",
      "    }\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "import pandas as pd\n",
    "import re\n",
    "from typing import Dict, List, Any, Union\n",
    "\n",
    "# Import utilitas dari paket utils (sesuaikan dengan environment Anda)\n",
    "from chart_generator.functions import call_gemini\n",
    "\n",
    "def generate_executive_summary(TOPIC, START_DATE, END_DATE, SAVE_PATH):\n",
    "    \"\"\"\n",
    "    Membuat executive summary berdasarkan analisis data dari file JSON dan CSV \n",
    "    yang disimpan di SAVE_PATH.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    TOPIC : str\n",
    "        Topik utama yang dianalisis\n",
    "    START_DATE : str\n",
    "        Tanggal awal periode analisis\n",
    "    END_DATE : str\n",
    "        Tanggal akhir periode analisis\n",
    "    SAVE_PATH : str\n",
    "        Path direktori tempat semua file analisis disimpan\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    Dict\n",
    "        Executive summary dalam format JSON\n",
    "    \"\"\"\n",
    "    # Inisialisasi dictionary untuk menyimpan semua data\n",
    "    data = {}\n",
    "    \n",
    "    # 1. Load sentiment_breakdown.json\n",
    "    try:\n",
    "        with open(os.path.join(SAVE_PATH, 'sentiment_breakdown.json'), 'r') as f:\n",
    "            data['sentiment_counts'] = json.load(f)\n",
    "        print(\"Loaded sentiment breakdown data\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading sentiment breakdown: {e}\")\n",
    "        data['sentiment_counts'] = {}\n",
    "    \n",
    "    # 2. Load sentiment_by_categories.json\n",
    "    try:\n",
    "        with open(os.path.join(SAVE_PATH, 'sentiment_by_categories.json'), 'r') as f:\n",
    "            data['pivot_sentiment'] = json.load(f)\n",
    "        print(\"Loaded sentiment by categories data\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading sentiment by categories: {e}\")\n",
    "        data['pivot_sentiment'] = []\n",
    "    \n",
    "    # 3. Load presence_score_analysis.json\n",
    "    try:\n",
    "        with open(os.path.join(SAVE_PATH, 'presence_score_analysis.json'), 'r') as f:\n",
    "            data['presence_score_analysis'] = json.load(f)\n",
    "        print(\"Loaded presence score analysis\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading presence score analysis: {e}\")\n",
    "        data['presence_score_analysis'] = {}\n",
    "    \n",
    "    # 4. Load sentiment_analysis.json\n",
    "    try:\n",
    "        with open(os.path.join(SAVE_PATH, 'sentiment_analysis.json'), 'r') as f:\n",
    "            data['sentiment_analysis'] = json.load(f)\n",
    "        print(\"Loaded sentiment analysis\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading sentiment analysis: {e}\")\n",
    "        data['sentiment_analysis'] = {}\n",
    "    \n",
    "    # 5. Load popular_mentions.csv\n",
    "    try:\n",
    "        data['popular_mentions'] = pd.read_csv(os.path.join(SAVE_PATH, 'popular_mentions.csv')).to_dict(orient='records')[:5]  # Limit to top 5\n",
    "        print(\"Loaded popular mentions data\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading popular mentions: {e}\")\n",
    "        data['popular_mentions'] = []\n",
    "    \n",
    "    # 6. Load KOL data\n",
    "    try:\n",
    "        kol_data = []\n",
    "        with open(os.path.join(SAVE_PATH, 'kol.json'), 'r') as f:\n",
    "            for line in f:\n",
    "                kol_data.append(json.loads(line.strip()))\n",
    "        data['kol_data'] = kol_data[:5]  # Limit to top 5\n",
    "        print(\"Loaded KOL data\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading KOL data: {e}\")\n",
    "        data['kol_data'] = []\n",
    "    \n",
    "    # 7. Load topic_overview.json\n",
    "    try:\n",
    "        topic_data = []\n",
    "        with open(os.path.join(SAVE_PATH, 'topic_overview.json'), 'r') as f:\n",
    "            for line in f:\n",
    "                topic_data.append(json.loads(line.strip()))\n",
    "        data['top_entities'] = topic_data[:10]  # Limit to top 10\n",
    "        print(\"Loaded topic overview data\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading topic overview: {e}\")\n",
    "        data['top_entities'] = []\n",
    "    \n",
    "    # 8. Try to load recommendations.json if available\n",
    "    try:\n",
    "        with open(os.path.join(SAVE_PATH, 'recommendations.json'), 'r') as f:\n",
    "            data['recommendations'] = json.load(f)\n",
    "        print(\"Loaded recommendations data\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading recommendations: {e}\")\n",
    "        data['recommendations'] = []\n",
    "    \n",
    "    # Extract platform data from sentiment_by_categories.json for list of top platforms\n",
    "    top_platforms = []\n",
    "    if data['pivot_sentiment']:\n",
    "        # Sort by total_mentions and get top 5 platforms\n",
    "        sorted_platforms = sorted(data['pivot_sentiment'], key=lambda x: x.get('total_mentions', 0), reverse=True)\n",
    "        top_platforms = [p.get('channel', '') for p in sorted_platforms[:5] if p.get('channel')]\n",
    "    \n",
    "    # Calculate basic metrics for the summary\n",
    "    total_mentions = sum(data['sentiment_counts'].values()) if data['sentiment_counts'] else 0\n",
    "    \n",
    "    # Calculate sentiment percentages\n",
    "    sentiment_percentages = {}\n",
    "    if total_mentions > 0 and data['sentiment_counts']:\n",
    "        for sentiment, count in data['sentiment_counts'].items():\n",
    "            sentiment_percentages[sentiment] = round((count / total_mentions) * 100, 1)\n",
    "    \n",
    "    # Build the prompt\n",
    "    prompt = f\"\"\"\n",
    "    You are a senior data analyst specializing in social media insights. Create a comprehensive executive summary of data about [{TOPIC}] for the period [{START_DATE}] to [{END_DATE}].\n",
    "\n",
    "    ## SENTIMENT DATA\n",
    "    \n",
    "    Overall sentiment distribution:\n",
    "    {data['sentiment_counts']}\n",
    "    \n",
    "    Sentiment percentages:\n",
    "    {sentiment_percentages}\n",
    "    \n",
    "    Sentiment by platform/channel:\n",
    "    {data['pivot_sentiment'][:5] if data['pivot_sentiment'] else \"No data available\"}\n",
    "    \"\"\"\n",
    "    \n",
    "    # Add presence score analysis if available\n",
    "    if data['presence_score_analysis']:\n",
    "        prompt += f\"\"\"\n",
    "    ## PRESENCE SCORE ANALYSIS\n",
    "    \n",
    "    Overall presence score trends:\n",
    "    {data['presence_score_analysis']}\n",
    "    \"\"\"\n",
    "    \n",
    "    # Add sentiment analysis if available\n",
    "    if data['sentiment_analysis']:\n",
    "        prompt += f\"\"\"\n",
    "    ## DETAILED SENTIMENT ANALYSIS\n",
    "    \n",
    "    In-depth sentiment analysis:\n",
    "    {data['sentiment_analysis']}\n",
    "    \"\"\"\n",
    "    \n",
    "    # Add popular mentions if available\n",
    "    if data['popular_mentions']:\n",
    "        prompt += f\"\"\"\n",
    "    ## POPULAR MENTIONS\n",
    "    \n",
    "    Sample of popular posts:\n",
    "    {data['popular_mentions'][:3]}\n",
    "    \"\"\"\n",
    "    \n",
    "    # Add top entities if available\n",
    "    if data['top_entities']:\n",
    "        prompt += f\"\"\"\n",
    "    ## TOP ENTITIES/TOPICS\n",
    "    \n",
    "    Most discussed entities or subtopics:\n",
    "    {data['top_entities'][:8]}\n",
    "    \"\"\"\n",
    "    \n",
    "    # Add KOL data if available\n",
    "    if data['kol_data']:\n",
    "        prompt += f\"\"\"\n",
    "    ## KEY OPINION LEADERS\n",
    "    \n",
    "    Top influencers discussing this topic:\n",
    "    {data['kol_data'][:3]}\n",
    "    \"\"\"\n",
    "    \n",
    "    # Add list of top platforms\n",
    "    prompt += f\"\"\"\n",
    "    ## TOP PLATFORMS\n",
    "    \n",
    "    The most active platforms for this topic are:\n",
    "    {top_platforms if top_platforms else \"Data not available\"}\n",
    "    \"\"\"\n",
    "    \n",
    "    # Add recommendations if available\n",
    "    if data['recommendations']:\n",
    "        prompt += f\"\"\"\n",
    "    ## STRATEGIC RECOMMENDATIONS\n",
    "    \n",
    "    Key recommendations based on the data:\n",
    "    {data['recommendations']}\n",
    "    \"\"\"\n",
    "    \n",
    "    # Instruksi dan format output\n",
    "    prompt = f\"\"\"\n",
    "    You are a senior data analyst specializing in social media insights. Create a comprehensive executive summary of data about [{TOPIC}] for the period [{START_DATE}] to [{END_DATE}].\n",
    "\n",
    "    [Include all the data sections here as in the previous implementation]\n",
    "    \n",
    "    ## TASK\n",
    "    \n",
    "    Based on all the data above, create a concise executive summary about [{TOPIC}] discussion in social media for the period [{START_DATE}] to [{END_DATE}].\n",
    "\n",
    "    Format your response as a JSON object with the following structure:\n",
    "    \n",
    "    {{\n",
    "      \"summary\": {{\n",
    "        \"scope_and_sentiment\": {{\n",
    "          \"title\": \"Scope and Overall Sentiment:\",\n",
    "          \"points\": [\n",
    "            \"Point about total mentions with specific numbers and dominant sentiment\",\n",
    "            \"Point about negative mentions percentage, with context if possible\",\n",
    "            \"Point about the nature of coverage (factual, opinion-based, etc.)\"\n",
    "          ]\n",
    "        }},\n",
    "        \"dominant_topics\": {{\n",
    "          \"title\": \"Dominant Topics:\",\n",
    "          \"topics\": [\n",
    "            {{\n",
    "              \"name\": \"Topic name with highest reach\",\n",
    "              \"reach\": \"Specific reach number in millions if available\",\n",
    "              \"sentiment\": \"Sentiment percentage breakdown\",\n",
    "              \"key_points\": [\n",
    "                \"Important detail or projection about this topic\",\n",
    "                \"Additional context if relevant\"\n",
    "              ]\n",
    "            }},\n",
    "            // Include 3-5 more topics with similar structure\n",
    "          ]\n",
    "        }},\n",
    "        \"peak_periods\": {{\n",
    "          \"title\": \"Peak Periods:\",\n",
    "          \"points\": [\n",
    "            \"Specific date range with highest positive coverage with explanation\",\n",
    "            \"Specific date range with highest engagement with explanation if different\"\n",
    "          ]\n",
    "        }},\n",
    "        \"negative_sentiment\": {{\n",
    "          \"title\": \"Negative Sentiment Analysis:\",\n",
    "          \"mentions\": [\n",
    "            {{\n",
    "              \"source\": \"Source of negative mention\",\n",
    "              \"description\": \"Brief description of the criticism\"\n",
    "            }},\n",
    "            // Include all negative mentions if few, or categorize if many\n",
    "          ]\n",
    "        }},\n",
    "        \"key_recommendations\": {{\n",
    "          \"title\": \"Key Recommendations:\",\n",
    "          \"points\": [\n",
    "            \"Strategic recommendation 1 based on the data\",\n",
    "            \"Strategic recommendation 2 based on the data\",\n",
    "            \"Strategic recommendation 3 based on the data\"\n",
    "          ]\n",
    "        }}\n",
    "      }}\n",
    "    }}\n",
    "\n",
    "    The summary should be concise but comprehensive, with specific numbers and percentages where available. Use bullet-point style within the JSON structure for clarity and readability.\n",
    "\n",
    "    IMPORTANT: Ensure your output is ONLY the properly formatted JSON with no additional text, markdown, or explanatory content.\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"Prompt generated, calling LLM...\")\n",
    "    \n",
    "    try:\n",
    "        # Call LLM to generate executive summary\n",
    "        summary_text = call_gemini(prompt)\n",
    "        \n",
    "        # Try to parse JSON directly\n",
    "        try:\n",
    "            summary = json.loads(re.findall(r'\\{.*\\}',summary_text, flags = re.I|re.S)[0])\n",
    "        except json.JSONDecodeError:\n",
    "            # Use regex as fallback if direct parsing fails\n",
    "            print(\"Direct JSON parsing failed, trying regex extraction...\")\n",
    "            json_pattern = re.compile(r'\\{\\s*\"title\".*\\}\\s*\\}', re.DOTALL)\n",
    "            json_match = json_pattern.search(summary_text)\n",
    "            \n",
    "            if json_match:\n",
    "                json_str = json_match.group(0)\n",
    "                summary = json.loads(json_str)\n",
    "            else:\n",
    "                # Create fallback summary\n",
    "                print(\"JSON extraction failed, creating fallback summary...\")\n",
    "                summary = {\n",
    "                    \"title\": f\"Executive Summary: {TOPIC} {START_DATE} to {END_DATE}\",\n",
    "                    \"summary\": {\n",
    "                        \"overview\": \"Unable to generate comprehensive overview due to parsing issues.\",\n",
    "                        \"key_metrics\": [\n",
    "                            {\n",
    "                                \"name\": \"Total Mentions\",\n",
    "                                \"value\": total_mentions,\n",
    "                                \"insight\": \"Data available but parsing failed\"\n",
    "                            }\n",
    "                        ],\n",
    "                        \"sentiment_analysis\": {\n",
    "                            \"overall_sentiment\": \"Data available but parsing failed\",\n",
    "                            \"sentiment_trend\": \"Data available but parsing failed\",\n",
    "                            \"key_sentiment_drivers\": [\n",
    "                                \"Data available but parsing failed\"\n",
    "                            ]\n",
    "                        },\n",
    "                        \"platform_insights\": [],\n",
    "                        \"key_topics\": [],\n",
    "                        \"influencers\": [],\n",
    "                        \"timeline_highlights\": [],\n",
    "                        \"recommendations_summary\": []\n",
    "                    }\n",
    "                }\n",
    "        \n",
    "        # Save the executive summary to file\n",
    "        with open(os.path.join(SAVE_PATH, 'executive_summary.json'), 'w') as f:\n",
    "            json.dump(summary, f, indent=2)\n",
    "            \n",
    "        print(f\"Executive summary generated and saved to {os.path.join(SAVE_PATH, 'executive_summary.json')}\")\n",
    "        return summary\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(summary_text)\n",
    "        print(f\"Error generating executive summary: {e}\")\n",
    "        # Return fallback summary\n",
    "        fallback_summary = {\n",
    "            \"title\": f\"Executive Summary: {TOPIC} {START_DATE} to {END_DATE}\",\n",
    "            \"summary\": {\n",
    "                \"overview\": \"Unable to generate summary due to an error in processing.\",\n",
    "                \"error\": str(e)\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        with open(os.path.join(SAVE_PATH, 'executive_summary_error.json'), 'w') as f:\n",
    "            json.dump(fallback_summary, f, indent=2)\n",
    "            \n",
    "        return fallback_summary\n",
    "\n",
    "# Contoh penggunaan:\n",
    "if __name__ == \"__main__\":\n",
    "    summary = generate_executive_summary(\n",
    "        TOPIC='RUU TNI',\n",
    "        START_DATE=\"2025-04-01\",\n",
    "        END_DATE=\"2025-05-01\",\n",
    "        SAVE_PATH=SAVE_PATH\n",
    "    )\n",
    "    \n",
    "    print(json.dumps(summary, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a1c46cb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fdf1bf33",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# ES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25edee0a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-01T16:58:33.942262Z",
     "start_time": "2025-05-01T16:58:33.915469Z"
    },
    "code_folding": [
     14,
     41,
     68,
     134,
     227
    ],
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "from typing import Dict, List, Optional, Union, Any\n",
    "from chart_generator.functions import *\n",
    "from dotenv import load_dotenv\n",
    "from elasticsearch import Elasticsearch\n",
    "load_dotenv()  \n",
    "\n",
    "es = Elasticsearch(os.getenv('ES_HOST',\"http://34.101.178.71:9200/\"),\n",
    "    basic_auth=(os.getenv(\"ES_USERNAME\",\"elastic\"),os.getenv('ES_PASSWORD',\"elasticpassword\"))  # Sesuaikan dengan kredensial Anda\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "316d7285",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-02T03:19:17.828490Z",
     "start_time": "2025-05-02T03:19:17.682702Z"
    },
    "code_folding": [
     13
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "from typing import Dict, List, Optional, Union, Any\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "from elasticsearch import Elasticsearch\n",
    "load_dotenv()  \n",
    "\n",
    "es = Elasticsearch(os.getenv('ES_HOST',\"http://34.101.178.71:9200/\"),\n",
    "    basic_auth=(os.getenv(\"ES_USERNAME\",\"elastic\"),os.getenv('ES_PASSWORD',\"elasticpassword\"))  # Sesuaikan dengan kredensial Anda\n",
    "        )\n",
    "\n",
    "def get_data(KEYWORDS, START_DATE, END_DATE):\n",
    "    \"\"\"\n",
    "    Menghitung presence score dari data Elasticsearch menggunakan aggregation.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    KEYWORDS : List[str]\n",
    "        Daftar keyword untuk filter\n",
    "    START_DATE : str\n",
    "        Tanggal awal periode (YYYY-MM-DD)\n",
    "    END_DATE : str\n",
    "        Tanggal akhir periode (YYYY-MM-DD)\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    pd.DataFrame\n",
    "        DataFrame dengan kolom date dan presence_score\n",
    "    \"\"\"\n",
    "\n",
    "    # Definisikan semua channel dan indeks\n",
    "    default_channels = ['reddit', 'youtube', 'linkedin', 'twitter', \n",
    "                        'tiktok', 'instagram', 'facebook', 'news', 'threads']\n",
    "    indices = [f\"{ch}_data\" for ch in default_channels]\n",
    "    \n",
    "    # Build keyword filter \n",
    "    keyword_conditions = []\n",
    "    for kw in KEYWORDS:\n",
    "        keyword_conditions.append({\"match\": {\"post_caption\": {\"query\": kw, \"operator\": \"AND\"}}})\n",
    "        keyword_conditions.append({\"match\": {\"issue\": {\"query\": kw, \"operator\": \"AND\"}}})\n",
    "    \n",
    "    keyword_filter = {\n",
    "        \"bool\": {\n",
    "            \"should\": keyword_conditions,\n",
    "            \"minimum_should_match\": 1\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # Bangun query dengan date_histogram aggregation\n",
    "    # untuk mendapatkan total_mentions, total_reach, dan total_engagement per hari\n",
    "    query = {\n",
    "        \"size\": 0,\n",
    "        \"query\": {\n",
    "            \"bool\": {\n",
    "                \"must\": [\n",
    "                    {\n",
    "                        \"range\": {\n",
    "                            \"post_created_at\": {\n",
    "                                \"gte\": START_DATE,\n",
    "                                \"lte\": END_DATE\n",
    "                            }\n",
    "                        }\n",
    "                    },\n",
    "                    keyword_filter\n",
    "                ]\n",
    "            }\n",
    "        },\n",
    "        \"aggs\": {\n",
    "            \"data_per_day\": {\n",
    "                \"date_histogram\": {\n",
    "                    \"field\": \"post_created_at\",\n",
    "                    \"calendar_interval\": \"day\",\n",
    "                    \"format\": \"yyyy-MM-dd\",\n",
    "                    \"min_doc_count\": 0,\n",
    "                    \"extended_bounds\": {\n",
    "                        \"min\": START_DATE,\n",
    "                        \"max\": END_DATE\n",
    "                    }\n",
    "                },\n",
    "                \"aggs\": {\n",
    "                    \"total_reach\": {\n",
    "                        \"sum\": {\n",
    "                            \"field\": \"reach_score\"\n",
    "                        }\n",
    "                    },\n",
    "                    \"total_engagement\": {\n",
    "                        \"sum\": {\n",
    "                            \"script\": {\n",
    "                                \"source\": \"\"\"\n",
    "                                    def likes = doc.containsKey('likes') && !doc['likes'].empty ? doc['likes'].value : 0;\n",
    "                                    def shares = doc.containsKey('shares') && !doc['shares'].empty ? doc['shares'].value : 0;\n",
    "                                    def comments = doc.containsKey('comments') && !doc['comments'].empty ? doc['comments'].value : 0;\n",
    "                                    def favorites = doc.containsKey('favorites') && !doc['favorites'].empty ? doc['favorites'].value : 0;\n",
    "                                    def views = doc.containsKey('views') && !doc['views'].empty ? doc['views'].value : 0;\n",
    "                                    def retweets = doc.containsKey('retweets') && !doc['retweets'].empty ? doc['retweets'].value : 0;\n",
    "                                    def replies = doc.containsKey('replies') && !doc['replies'].empty ? doc['replies'].value : 0;\n",
    "                                    def reposts = doc.containsKey('reposts') && !doc['reposts'].empty ? doc['reposts'].value : 0;\n",
    "                                    def votes = doc.containsKey('votes') && !doc['votes'].empty ? doc['votes'].value : 0;\n",
    "                                    \n",
    "                                    return likes + shares + comments + favorites + views + retweets + replies + reposts + votes;\n",
    "                                \"\"\"\n",
    "                            }\n",
    "                        }\n",
    "                    }\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # Execute query\n",
    "    response = es.search(\n",
    "        index=\",\".join(indices),\n",
    "        body=query\n",
    "    )\n",
    "    \n",
    "    # Ekstrak data dari respons\n",
    "    buckets = response[\"aggregations\"][\"data_per_day\"][\"buckets\"]\n",
    "    \n",
    "    # Siapkan data untuk DataFrame\n",
    "    topic_data = []\n",
    "    for bucket in buckets:\n",
    "        date = bucket[\"key_as_string\"]\n",
    "        total_mentions = bucket[\"doc_count\"]\n",
    "        total_reach = bucket[\"total_reach\"][\"value\"]\n",
    "        total_engagement = bucket[\"total_engagement\"][\"value\"]\n",
    "        \n",
    "        topic_data.append({\n",
    "            \"date\": date,\n",
    "            \"total_mentions\": total_mentions,\n",
    "            \"total_reach\": total_reach,\n",
    "            \"total_engagement\": total_engagement\n",
    "        })\n",
    "    \n",
    "    # Konversi ke DataFrame\n",
    "    df_topic_data = pd.DataFrame(topic_data)\n",
    "    \n",
    "    # Jika tidak ada data, kembalikan DataFrame kosong\n",
    "    if df_topic_data.empty:\n",
    "        return pd.DataFrame(columns=[\"date\", \"presence_score\"])\n",
    "    \n",
    "    # Hitung nilai maksimum\n",
    "    max_mentions = df_topic_data[\"total_mentions\"].max()\n",
    "    max_reach = df_topic_data[\"total_reach\"].max()\n",
    "    max_engagement = df_topic_data[\"total_engagement\"].max()\n",
    "    \n",
    "    # Hitung presence_score\n",
    "    df_topic_data[\"presence_score\"] = df_topic_data.apply(\n",
    "        lambda row: round(\n",
    "            ((row[\"total_mentions\"] / max_mentions if max_mentions else 0) * 40) +\n",
    "            ((row[\"total_reach\"] / max_reach if max_reach else 0) * 40) +\n",
    "            ((row[\"total_engagement\"] / max_engagement if max_engagement else 0) * 20),\n",
    "            2\n",
    "        ),\n",
    "        axis=1\n",
    "    )\n",
    "    \n",
    "    # Pilih kolom yang diperlukan dan urutkan berdasarkan tanggal\n",
    "    result_df = df_topic_data[[\"date\", \"presence_score\"]].sort_values(by=\"date\")\n",
    "    \n",
    "    # Pastikan date dalam format string\n",
    "    result_df[\"date\"] = result_df[\"date\"].astype(str)\n",
    "    \n",
    "    return result_df\n",
    "\n",
    "# Contoh penggunaan:\n",
    "if __name__ == \"__main__\":\n",
    "    # Definisikan parameter\n",
    "    KEYWORDS = ['APBN', 'Anggaran Pendapatan dan Belanja Negara', 'RAPBN', 'Kementerian Keuangan', \n",
    "               'Sri Mulyani', 'Defisit APBN', 'Belanja Negara', 'Pendapatan Negara', 'Pajak', \n",
    "               'Utang Negara', 'Subsidi', 'Transfer Daerah', 'Dana Desa', 'Postur APBN', 'APBN 2024', 'apbn']\n",
    "    \n",
    "    START_DATE = '2025-04-01'\n",
    "    END_DATE = '2025-05-01'\n",
    "    \n",
    "    # Mendapatkan data presence score\n",
    "    presence_score = get_data(\n",
    "        KEYWORDS=KEYWORDS,\n",
    "        START_DATE=START_DATE,\n",
    "        END_DATE=END_DATE\n",
    "    )\n",
    "    \n",
    "    # Tampilkan hasil\n",
    "    if not presence_score.empty:\n",
    "        print(presence_score.head(10))\n",
    "    else:\n",
    "        print(\"No data found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02fd93e2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-02T03:19:58.473142Z",
     "start_time": "2025-05-02T03:19:58.467017Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "high_presence_date = presence_score.sort_values('presence_score', ascending = False)['date'].to_list()[0]\n",
    "high_presence_date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a20e5c55",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-02T03:20:08.054583Z",
     "start_time": "2025-05-02T03:20:08.027947Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from utils.list_of_mentions import get_filtered_mentions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a9e742f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-02T03:21:27.635587Z",
     "start_time": "2025-05-02T03:21:19.345289Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "query = f\"\"\"\n",
    "WITH \n",
    "metric_data AS (\n",
    "    SELECT \n",
    "        sentiment, \n",
    "        a.channel, \n",
    "        COUNT(*) AS total_mentions\n",
    "    FROM medsos.post_analysis a\n",
    "    JOIN medsos.post_category c\n",
    "    ON a.link_post = c.link_post\n",
    "    WHERE {FILTER_KEYWORD}\n",
    "    AND a.post_created_at BETWEEN '{high_presence_date} 00:00:00' AND '{high_presence_date} 29:59:59'\n",
    "    GROUP BY 1, 2\n",
    "),\n",
    "post_data AS (\n",
    "    SELECT * from (select\n",
    "        a.post_caption, \n",
    "        a.channel, \n",
    "        c.issue, \n",
    "        sentiment,\n",
    "        reach_score, \n",
    "        viral_score, \n",
    "        (COALESCE(likes, 0) + COALESCE(shares, 0) + COALESCE(comments, 0) + COALESCE(favorites, 0)\n",
    "         + COALESCE(views, 0) + COALESCE(retweets, 0) + COALESCE(replies, 0) \n",
    "         + COALESCE(reposts, 0) + COALESCE(votes, 0)) AS engagement,\n",
    "        a.link_post\n",
    "    FROM medsos.post_analysis a\n",
    "    JOIN medsos.post_category c\n",
    "    ON a.link_post = c.link_post\n",
    "    WHERE {FILTER_KEYWORD}\n",
    "    AND a.post_created_at BETWEEN '{high_presence_date} 00:00:00' AND '{high_presence_date} 29:59:59'\n",
    "    AND LOWER(c.issue) NOT IN ('not specified'))\n",
    "    order by (reach_score + viral_score + engagement) desc\n",
    "    limit 100\n",
    ")\n",
    "-- Gabungkan hasil dari kedua CTE dengan UNION ALL\n",
    "SELECT \n",
    "    'metrics' AS data_type,\n",
    "    sentiment,\n",
    "    channel,\n",
    "    total_mentions,\n",
    "    NULL AS post_caption,\n",
    "    NULL AS issue,\n",
    "    NULL AS reach_score,\n",
    "    NULL AS viral_score,\n",
    "    NULL AS engagement,\n",
    "    NULL AS link_post\n",
    "FROM metric_data\n",
    "\n",
    "UNION ALL\n",
    "\n",
    "SELECT \n",
    "    'posts' AS data_type,\n",
    "    sentiment,\n",
    "    channel,\n",
    "    NULL AS total_mentions,\n",
    "    post_caption,\n",
    "    issue,\n",
    "    reach_score,\n",
    "    viral_score,\n",
    "    engagement,\n",
    "    link_post\n",
    "FROM post_data\n",
    "ORDER BY data_type, \n",
    "    CASE WHEN data_type = 'posts' THEN (reach_score + viral_score + engagement) END DESC\n",
    "\"\"\"\n",
    "\n",
    "# Eksekusi query gabungan\n",
    "combined_results = BQ.to_pull_data(query)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f75df8e6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-02T03:21:33.184452Z",
     "start_time": "2025-05-02T03:21:33.166079Z"
    },
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "combined_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18da12f3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-02T03:22:38.121356Z",
     "start_time": "2025-05-02T03:22:38.110358Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Pisahkan hasil berdasarkan data_type\n",
    "metric_presence = combined_results[combined_results['data_type'] == 'metrics']\n",
    "sample_post_presence = combined_results[combined_results['data_type'] == 'posts']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee23d02a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-02T03:22:47.927172Z",
     "start_time": "2025-05-02T03:22:47.910960Z"
    },
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "metric_presence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1819e81f",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a18ef79",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30a945c8",
   "metadata": {
    "code_folding": [
     6
    ],
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d998c357",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-02T04:03:19.289674Z",
     "start_time": "2025-05-02T04:03:18.884124Z"
    },
    "code_folding": [
     6
    ],
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cb4749f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-02T04:03:24.822876Z",
     "start_time": "2025-05-02T04:03:24.812031Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41098ef1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-02T04:03:38.639025Z",
     "start_time": "2025-05-02T04:03:38.624851Z"
    },
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "combined_results[combined_results[\"data_type\"] == \"posts\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2359d3b0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-02T04:10:17.372986Z",
     "start_time": "2025-05-02T04:10:17.331212Z"
    },
    "code_folding": [
     15,
     50,
     111,
     264,
     486
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "from typing import Dict, List, Optional, Union, Any\n",
    "from chart_generator.functions import *\n",
    "from dotenv import load_dotenv\n",
    "from elasticsearch import Elasticsearch\n",
    "\n",
    "load_dotenv()  \n",
    "\n",
    "es = Elasticsearch(os.getenv('ES_HOST',\"http://34.101.178.71:9200/\"),\n",
    "    basic_auth=(os.getenv(\"ES_USERNAME\",\"elastic\"),os.getenv('ES_PASSWORD',\"elasticpassword\"))  # Sesuaikan dengan kredensial Anda\n",
    "        )\n",
    "\n",
    "\n",
    "def plot_donut_score(score, title=\"How much attention\\na topic or figure gets\", save_path=None):\n",
    "    \"\"\"\n",
    "    Create a donut chart with score (0-100) in the center.\n",
    "    \n",
    "    Parameters:\n",
    "        score (float): Score between 0 and 100\n",
    "        title (str): Text shown below the chart\n",
    "        save_path (str): Optional file path to save the chart\n",
    "    \"\"\"\n",
    "    # Colors\n",
    "    main_color = '#1a73e8'\n",
    "    bg_color = '#e0e0e0'\n",
    "\n",
    "    # Create figure\n",
    "    fig, ax = plt.subplots(figsize=(3, 3))\n",
    "    ax.pie([score, 100 - score],\n",
    "           startangle=90,\n",
    "           colors=[main_color, bg_color],\n",
    "           wedgeprops=dict(width=0.42),\n",
    "           counterclock=False)\n",
    "\n",
    "    # Center text\n",
    "    ax.text(0, 0, f\"{int(round(score))}\", ha='center', va='center', fontsize=40, color='#333333')\n",
    "\n",
    "    # Title below the donut\n",
    "    plt.text(0, -1.3, title, ha='center', va='center', fontsize=12, color='#757575')\n",
    "\n",
    "    # Remove all axes\n",
    "    ax.set_aspect('equal')\n",
    "    plt.axis('off')\n",
    "    plt.tight_layout()\n",
    "\n",
    "    if save_path:\n",
    "        plt.savefig(save_path, bbox_inches='tight', dpi=150, transparent=True)\n",
    "\n",
    "def plot_presence_score_trend(data, title=\"Your presence score\", save_path=None,\n",
    "                              show_dots=True, color_theme='#1a73e8'):\n",
    "    df = pd.DataFrame(data)\n",
    "    df['date'] = pd.to_datetime(df['date'])\n",
    "    df = df.sort_values('date')\n",
    "\n",
    "    x = df['date']\n",
    "    y = df['presence_score'].values\n",
    "\n",
    "    # Smooth\n",
    "    def smooth(x, y):\n",
    "        if len(x) < 4:\n",
    "            return x, y\n",
    "        x_numeric = np.arange(len(x))\n",
    "        spl = make_interp_spline(x_numeric, y, k=3)\n",
    "        xnew = np.linspace(x_numeric.min(), x_numeric.max(), 300)\n",
    "        ynew = spl(xnew)\n",
    "        x_interp = np.interp(xnew, x_numeric, x.astype(np.int64) // 10**9)\n",
    "        x_interp = pd.to_datetime(x_interp, unit='s')\n",
    "        return x_interp, ynew\n",
    "\n",
    "    x_smooth, y_smooth = smooth(x, y)\n",
    "\n",
    "    # Plot\n",
    "    fig, ax = plt.subplots(figsize=(12, 4))  # 33.35cm x 9.14cm\n",
    "    ax.set_facecolor('#ffffff')\n",
    "    fig.patch.set_facecolor('#ffffff')\n",
    "\n",
    "    ax.plot(x_smooth, y_smooth, color=color_theme, label=title, linewidth=2)\n",
    "\n",
    "    if show_dots:\n",
    "        ax.scatter(x, y, color=color_theme, s=15, zorder=3)\n",
    "\n",
    "    # Clean styling\n",
    "    for spine in ax.spines.values():\n",
    "        spine.set_visible(False)\n",
    "\n",
    "    ax.tick_params(axis='y', colors=color_theme)\n",
    "    ax.yaxis.label.set_color(color_theme)\n",
    "    ax.grid(axis='y', linestyle='-', linewidth=0.5, alpha=0.3)\n",
    "\n",
    "    ax.xaxis.set_major_formatter(DateFormatter('%d %b %Y'))\n",
    "    plt.xticks(rotation=0)\n",
    "    ax.xaxis.label.set_color('#5f6368')\n",
    "\n",
    "    # Annotate peak\n",
    "    peak_idx = np.argmax(y)\n",
    "    peak_date = x.iloc[peak_idx]\n",
    "    peak_value = y[peak_idx]\n",
    "    label = f\"{int(round(peak_value))} on {peak_date.strftime('%d %b %Y')}\"\n",
    "    ax.annotate(label, xy=(peak_date, peak_value), xytext=(peak_date, peak_value + 5),\n",
    "                ha='center', fontsize=9, color=color_theme,\n",
    "                arrowprops=dict(arrowstyle='-', color=color_theme, lw=1))\n",
    "\n",
    "    # Legend\n",
    "    ax.legend(loc='best', frameon=False)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    if save_path:\n",
    "        plt.savefig(save_path, bbox_inches='tight', dpi=150, transparent=True)\n",
    "        \n",
    "def get_data(KEYWORDS, START_DATE, END_DATE):\n",
    "    \"\"\"\n",
    "    Menghitung presence score dari data Elasticsearch menggunakan aggregation.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    KEYWORDS : List[str]\n",
    "        Daftar keyword untuk filter\n",
    "    START_DATE : str\n",
    "        Tanggal awal periode (YYYY-MM-DD)\n",
    "    END_DATE : str\n",
    "        Tanggal akhir periode (YYYY-MM-DD)\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    pd.DataFrame\n",
    "        DataFrame dengan kolom date dan presence_score\n",
    "    \"\"\"\n",
    "\n",
    "    # Definisikan semua channel dan indeks\n",
    "    default_channels = ['reddit', 'youtube', 'linkedin', 'twitter', \n",
    "                        'tiktok', 'instagram', 'facebook', 'news', 'threads']\n",
    "    indices = [f\"{ch}_data\" for ch in default_channels]\n",
    "    \n",
    "    # Build keyword filter \n",
    "    keyword_conditions = []\n",
    "    for kw in KEYWORDS:\n",
    "        keyword_conditions.append({\"match\": {\"post_caption\": {\"query\": kw, \"operator\": \"AND\"}}})\n",
    "        keyword_conditions.append({\"match\": {\"issue\": {\"query\": kw, \"operator\": \"AND\"}}})\n",
    "    \n",
    "    keyword_filter = {\n",
    "        \"bool\": {\n",
    "            \"should\": keyword_conditions,\n",
    "            \"minimum_should_match\": 1\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # Bangun query dengan date_histogram aggregation\n",
    "    # untuk mendapatkan total_mentions, total_reach, dan total_engagement per hari\n",
    "    query = {\n",
    "        \"size\": 0,\n",
    "        \"query\": {\n",
    "            \"bool\": {\n",
    "                \"must\": [\n",
    "                    {\n",
    "                        \"range\": {\n",
    "                            \"post_created_at\": {\n",
    "                                \"gte\": START_DATE,\n",
    "                                \"lte\": END_DATE\n",
    "                            }\n",
    "                        }\n",
    "                    },\n",
    "                    keyword_filter\n",
    "                ]\n",
    "            }\n",
    "        },\n",
    "        \"aggs\": {\n",
    "            \"data_per_day\": {\n",
    "                \"date_histogram\": {\n",
    "                    \"field\": \"post_created_at\",\n",
    "                    \"calendar_interval\": \"day\",\n",
    "                    \"format\": \"yyyy-MM-dd\",\n",
    "                    \"min_doc_count\": 0,\n",
    "                    \"extended_bounds\": {\n",
    "                        \"min\": START_DATE,\n",
    "                        \"max\": END_DATE\n",
    "                    }\n",
    "                },\n",
    "                \"aggs\": {\n",
    "                    \"total_reach\": {\n",
    "                        \"sum\": {\n",
    "                            \"field\": \"reach_score\"\n",
    "                        }\n",
    "                    },\n",
    "                    \"total_engagement\": {\n",
    "                        \"sum\": {\n",
    "                            \"script\": {\n",
    "                                \"source\": \"\"\"\n",
    "                                    def likes = doc.containsKey('likes') && !doc['likes'].empty ? doc['likes'].value : 0;\n",
    "                                    def shares = doc.containsKey('shares') && !doc['shares'].empty ? doc['shares'].value : 0;\n",
    "                                    def comments = doc.containsKey('comments') && !doc['comments'].empty ? doc['comments'].value : 0;\n",
    "                                    def favorites = doc.containsKey('favorites') && !doc['favorites'].empty ? doc['favorites'].value : 0;\n",
    "                                    def views = doc.containsKey('views') && !doc['views'].empty ? doc['views'].value : 0;\n",
    "                                    def retweets = doc.containsKey('retweets') && !doc['retweets'].empty ? doc['retweets'].value : 0;\n",
    "                                    def replies = doc.containsKey('replies') && !doc['replies'].empty ? doc['replies'].value : 0;\n",
    "                                    def reposts = doc.containsKey('reposts') && !doc['reposts'].empty ? doc['reposts'].value : 0;\n",
    "                                    def votes = doc.containsKey('votes') && !doc['votes'].empty ? doc['votes'].value : 0;\n",
    "                                    \n",
    "                                    return likes + shares + comments + favorites + views + retweets + replies + reposts + votes;\n",
    "                                \"\"\"\n",
    "                            }\n",
    "                        }\n",
    "                    }\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # Execute query\n",
    "    response = es.search(\n",
    "        index=\",\".join(indices),\n",
    "        body=query\n",
    "    )\n",
    "    \n",
    "    # Ekstrak data dari respons\n",
    "    buckets = response[\"aggregations\"][\"data_per_day\"][\"buckets\"]\n",
    "    \n",
    "    # Siapkan data untuk DataFrame\n",
    "    topic_data = []\n",
    "    for bucket in buckets:\n",
    "        date = bucket[\"key_as_string\"]\n",
    "        total_mentions = bucket[\"doc_count\"]\n",
    "        total_reach = bucket[\"total_reach\"][\"value\"]\n",
    "        total_engagement = bucket[\"total_engagement\"][\"value\"]\n",
    "        \n",
    "        topic_data.append({\n",
    "            \"date\": date,\n",
    "            \"total_mentions\": total_mentions,\n",
    "            \"total_reach\": total_reach,\n",
    "            \"total_engagement\": total_engagement\n",
    "        })\n",
    "    \n",
    "    # Konversi ke DataFrame\n",
    "    df_topic_data = pd.DataFrame(topic_data)\n",
    "    \n",
    "    # Jika tidak ada data, kembalikan DataFrame kosong\n",
    "    if df_topic_data.empty:\n",
    "        return pd.DataFrame(columns=[\"date\", \"presence_score\"])\n",
    "    \n",
    "    # Hitung nilai maksimum\n",
    "    max_mentions = df_topic_data[\"total_mentions\"].max()\n",
    "    max_reach = df_topic_data[\"total_reach\"].max()\n",
    "    max_engagement = df_topic_data[\"total_engagement\"].max()\n",
    "    \n",
    "    # Hitung presence_score\n",
    "    df_topic_data[\"presence_score\"] = df_topic_data.apply(\n",
    "        lambda row: round(\n",
    "            ((row[\"total_mentions\"] / max_mentions if max_mentions else 0) * 40) +\n",
    "            ((row[\"total_reach\"] / max_reach if max_reach else 0) * 40) +\n",
    "            ((row[\"total_engagement\"] / max_engagement if max_engagement else 0) * 20),\n",
    "            2\n",
    "        ),\n",
    "        axis=1\n",
    "    )\n",
    "    \n",
    "    # Pilih kolom yang diperlukan dan urutkan berdasarkan tanggal\n",
    "    result_df = df_topic_data[[\"date\", \"presence_score\"]].sort_values(by=\"date\")\n",
    "    \n",
    "    # Pastikan date dalam format string\n",
    "    result_df[\"date\"] = result_df[\"date\"].astype(str)\n",
    "    \n",
    "    return result_df\n",
    "\n",
    "def get_metrics_and_posts(FILTER_KEYWORD, high_presence_date):\n",
    "    \"\"\"\n",
    "    Mendapatkan aggregate dari sentiment per channel dan sampling data post\n",
    "    menggunakan satu query Elasticsearch untuk efisiensi.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    FILTER_KEYWORD : List[str]\n",
    "        Daftar keyword untuk filter\n",
    "    high_presence_date : str\n",
    "        Tanggal dengan presence tinggi yang akan dianalisis (YYYY-MM-DD)\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    pd.DataFrame\n",
    "        DataFrame gabungan yang berisi metrics dan posts\n",
    "    \"\"\"\n",
    "\n",
    "    \n",
    "    # Definisikan semua channel dan indeks\n",
    "    default_channels = ['reddit', 'youtube', 'linkedin', 'twitter', \n",
    "                        'tiktok', 'instagram', 'facebook', 'news', 'threads']\n",
    "    indices = [f\"{ch}_data\" for ch in default_channels]\n",
    "    \n",
    "    # Format tanggal untuk query\n",
    "    start_datetime = f\"{high_presence_date} 00:00:00\"\n",
    "    end_datetime = f\"{high_presence_date} 23:59:59\"\n",
    "    \n",
    "    # Build keyword filter \n",
    "    keyword_conditions = []\n",
    "    for kw in FILTER_KEYWORD:\n",
    "        keyword_conditions.append({\"match\": {\"post_caption\": {\"query\": kw, \"operator\": \"AND\"}}})\n",
    "        keyword_conditions.append({\"match\": {\"issue\": {\"query\": kw, \"operator\": \"AND\"}}})\n",
    "    \n",
    "    keyword_filter = {\n",
    "        \"bool\": {\n",
    "            \"should\": keyword_conditions,\n",
    "            \"minimum_should_match\": 1\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # === QUERY 1: Metrics Data ===\n",
    "    # Query untuk mendapatkan aggregate dari sentiment per channel\n",
    "    metrics_query = {\n",
    "        \"size\": 0,\n",
    "        \"query\": {\n",
    "            \"bool\": {\n",
    "                \"must\": [\n",
    "                    {\n",
    "                        \"range\": {\n",
    "                            \"post_created_at\": {\n",
    "                                \"gte\": start_datetime,\n",
    "                                \"lte\": end_datetime\n",
    "                            }\n",
    "                        }\n",
    "                    },\n",
    "                    keyword_filter\n",
    "                ]\n",
    "            }\n",
    "        },\n",
    "        \"aggs\": {\n",
    "            \"sentiment_channel\": {\n",
    "                \"terms\": {\n",
    "                    \"field\": \"sentiment\",\n",
    "                    \"size\": 10  # Jumlah sentiment teratas\n",
    "                },\n",
    "                \"aggs\": {\n",
    "                    \"channels\": {\n",
    "                        \"terms\": {\n",
    "                            \"field\": \"channel\",\n",
    "                            \"size\": 20  # Jumlah channel teratas\n",
    "                        }\n",
    "                    }\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # === QUERY 2: Posts Data ===\n",
    "    # Query untuk mendapatkan sampling data post\n",
    "    posts_query = {\n",
    "        \"size\": 100,  # Limit 100 post teratas\n",
    "        \"query\": {\n",
    "            \"bool\": {\n",
    "                \"must\": [\n",
    "                    {\n",
    "                        \"range\": {\n",
    "                            \"post_created_at\": {\n",
    "                                \"gte\": start_datetime,\n",
    "                                \"lte\": end_datetime\n",
    "                            }\n",
    "                        }\n",
    "                    },\n",
    "                    keyword_filter\n",
    "                ],\n",
    "                \"must_not\": [\n",
    "                    {\n",
    "                        \"terms\": {\n",
    "                            \"issue.keyword\": [\"not specified\", \"Not Specified\", \"NOT SPECIFIED\"]\n",
    "                        }\n",
    "                    }\n",
    "                ]\n",
    "            }\n",
    "        },\n",
    "        \"_source\": [\n",
    "            \"post_caption\", \"channel\", \"issue\", \"sentiment\", \"reach_score\", \n",
    "            \"viral_score\", \"likes\", \"shares\", \"comments\", \"favorites\", \n",
    "            \"views\", \"retweets\", \"replies\", \"reposts\", \"votes\", \"link_post\"\n",
    "        ],\n",
    "        \"sort\": [\n",
    "            {\n",
    "                \"_script\": {\n",
    "                    \"type\": \"number\",\n",
    "                    \"script\": {\n",
    "                        \"source\": \"\"\"\n",
    "                            def reach = doc.containsKey('reach_score') && !doc['reach_score'].empty ? doc['reach_score'].value : 0;\n",
    "                            def viral = doc.containsKey('viral_score') && !doc['viral_score'].empty ? doc['viral_score'].value : 0;\n",
    "                            def likes = doc.containsKey('likes') && !doc['likes'].empty ? doc['likes'].value : 0;\n",
    "                            def shares = doc.containsKey('shares') && !doc['shares'].empty ? doc['shares'].value : 0;\n",
    "                            def comments = doc.containsKey('comments') && !doc['comments'].empty ? doc['comments'].value : 0;\n",
    "                            def favorites = doc.containsKey('favorites') && !doc['favorites'].empty ? doc['favorites'].value : 0;\n",
    "                            def views = doc.containsKey('views') && !doc['views'].empty ? doc['views'].value : 0;\n",
    "                            def retweets = doc.containsKey('retweets') && !doc['retweets'].empty ? doc['retweets'].value : 0;\n",
    "                            def replies = doc.containsKey('replies') && !doc['replies'].empty ? doc['replies'].value : 0;\n",
    "                            def reposts = doc.containsKey('reposts') && !doc['reposts'].empty ? doc['reposts'].value : 0;\n",
    "                            def votes = doc.containsKey('votes') && !doc['votes'].empty ? doc['votes'].value : 0;\n",
    "                            \n",
    "                            def engagement = likes + shares + comments + favorites + views + retweets + replies + reposts + votes;\n",
    "                            \n",
    "                            return reach + viral + engagement;\n",
    "                        \"\"\"\n",
    "                    },\n",
    "                    \"order\": \"desc\"\n",
    "                }\n",
    "            }\n",
    "        ]\n",
    "    }\n",
    "    \n",
    "    # Execute kedua query secara terpisah\n",
    "    metrics_response = es.search(\n",
    "        index=\",\".join(indices),\n",
    "        body=metrics_query\n",
    "    )\n",
    "    \n",
    "    posts_response = es.search(\n",
    "        index=\",\".join(indices),\n",
    "        body=posts_query\n",
    "    )\n",
    "    \n",
    "    # Proses results untuk metrics_data\n",
    "    metrics_data = []\n",
    "    sentiment_buckets = metrics_response[\"aggregations\"][\"sentiment_channel\"][\"buckets\"]\n",
    "    \n",
    "    for sentiment_bucket in sentiment_buckets:\n",
    "        sentiment = sentiment_bucket[\"key\"]\n",
    "        channel_buckets = sentiment_bucket[\"channels\"][\"buckets\"]\n",
    "        \n",
    "        for channel_bucket in channel_buckets:\n",
    "            channel = channel_bucket[\"key\"]\n",
    "            total_mentions = channel_bucket[\"doc_count\"]\n",
    "            \n",
    "            metrics_data.append({\n",
    "                \"data_type\": \"metrics\",\n",
    "                \"sentiment\": sentiment,\n",
    "                \"channel\": channel,\n",
    "                \"total_mentions\": total_mentions,\n",
    "                \"post_caption\": None,\n",
    "                \"issue\": None,\n",
    "                \"reach_score\": None,\n",
    "                \"viral_score\": None,\n",
    "                \"engagement\": None,\n",
    "                \"link_post\": None\n",
    "            })\n",
    "    \n",
    "    # Proses results untuk posts_data\n",
    "    posts_data = []\n",
    "    post_hits = posts_response[\"hits\"][\"hits\"]\n",
    "    \n",
    "    for hit in post_hits:\n",
    "        source = hit[\"_source\"]\n",
    "        \n",
    "        # Hitung total engagement\n",
    "        engagement = sum([\n",
    "            source.get(\"likes\", 0) or 0,\n",
    "            source.get(\"shares\", 0) or 0,\n",
    "            source.get(\"comments\", 0) or 0,\n",
    "            source.get(\"favorites\", 0) or 0,\n",
    "            source.get(\"views\", 0) or 0,\n",
    "            source.get(\"retweets\", 0) or 0,\n",
    "            source.get(\"replies\", 0) or 0,\n",
    "            source.get(\"reposts\", 0) or 0,\n",
    "            source.get(\"votes\", 0) or 0\n",
    "        ])\n",
    "        \n",
    "        posts_data.append({\n",
    "            \"data_type\": \"posts\",\n",
    "            \"sentiment\": source.get(\"sentiment\", \"\"),\n",
    "            \"channel\": source.get(\"channel\", \"\"),\n",
    "            \"total_mentions\": None,\n",
    "            \"post_caption\": source.get(\"post_caption\", \"\"),\n",
    "            \"issue\": source.get(\"issue\", \"\"),\n",
    "            \"reach_score\": source.get(\"reach_score\", 0),\n",
    "            \"viral_score\": source.get(\"viral_score\", 0),\n",
    "            \"engagement\": engagement,\n",
    "            \"link_post\": source.get(\"link_post\", \"\")\n",
    "        })\n",
    "    \n",
    "    # Gabungkan kedua hasil\n",
    "    combined_data = metrics_data + posts_data\n",
    "    \n",
    "    # Konversi ke DataFrame\n",
    "    df = pd.DataFrame(combined_data)\n",
    "    \n",
    "    # Urutkan hasil\n",
    "    df = df.sort_values(\n",
    "        by=[\"data_type\", \"reach_score\", \"viral_score\", \"engagement\"],\n",
    "        ascending=[True, False, False, False],\n",
    "        na_position='last'\n",
    "    )\n",
    "    \n",
    "    return df\n",
    "\n",
    "def presence_description(TOPIC, FILTER_KEYWORD, high_presence_date,SAVE_PATH):\n",
    "    \n",
    "    combined_results = get_metrics_and_posts(\n",
    "        FILTER_KEYWORD=FILTER_KEYWORD,\n",
    "        high_presence_date=high_presence_date\n",
    "    )\n",
    "\n",
    "    # Pisahkan hasil berdasarkan data_type\n",
    "    metric_presence = combined_results[combined_results['data_type'] == 'metrics']\n",
    "    sample_post_presence = combined_results[combined_results['data_type'] == 'posts']\n",
    "\n",
    "    #summarize\n",
    "    prompt = f\"\"\"\n",
    "    You are a media analyst assistant. Analyze a spike in presence score related to the topic [{TOPIC}] on {high_presence_date}.\n",
    "\n",
    "    Here is the supporting data:\n",
    "\n",
    "    1. **Sentiment Breakdown (total mentions)**  \n",
    "    {metric_presence.groupby('sentiment').sum().to_dict()['total_mentions']}\n",
    "\n",
    "    2. **Platform Breakdown (total mentions)**  \n",
    "    {metric_presence.groupby('channel').sum().to_dict()['total_mentions']}\n",
    "\n",
    "    3. **Top Posts on {high_presence_date}**  \n",
    "    {sample_post_presence[['post_caption','channel']].to_dict(orient='records')}\n",
    "\n",
    "    ---\n",
    "\n",
    "    **Instruction:**  \n",
    "    Summarize in **1 paragraph only** what likely caused the spike, which platform contributed the most, what sentiment dominated, and what topic(s) were most discussed — **only if relevant to [{TOPIC}]**. Use a professional, concise tone.\n",
    "    \"\"\"\n",
    "    summarize = call_gemini(prompt)\n",
    "\n",
    "    with open(os.path.join(SAVE_PATH,'presence_score_analysis.json'),'w') as f:\n",
    "        json.dump({'analysis':summarize},f)\n",
    "   \n",
    "def generatre_presence_score(TOPIC, KEYWORDS, START_DATE, END_DATE, SAVE_PATH):\n",
    "    \n",
    "    presence_score = get_data(KEYWORDS, START_DATE, END_DATE)\n",
    "    \n",
    "    plot_donut_score(presence_score['presence_score'].mean(),\n",
    "                     save_path = os.path.join(SAVE_PATH,'presence_score_donut.png'))\n",
    "\n",
    "    plot_presence_score_trend(presence_score.to_dict(orient = 'records'),\n",
    "                              save_path = os.path.join(SAVE_PATH,'presence_trend.png'))\n",
    "    \n",
    "    high_presence_date = presence_score.sort_values('presence_score', ascending = False)['date'].to_list()[0]\n",
    "    \n",
    "    presence_description(TOPIC, KEYWORDS, high_presence_date, SAVE_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bed088dd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-02T04:10:18.363346Z",
     "start_time": "2025-05-02T04:10:18.352322Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "\n",
    "KEYWORDS = ['APBN', 'Anggaran Pendapatan dan Belanja Negara', 'RAPBN', 'Kementerian Keuangan', 'Sri Mulyani', 'Defisit APBN', 'Belanja Negara', 'Pendapatan Negara', 'Pajak', 'Utang Negara', 'Subsidi', 'Transfer Daerah', 'Dana Desa', 'Postur APBN', 'APBN 2024', 'apbn']\n",
    "MAIN_TOPIC = 'APBN'\n",
    "\n",
    "start_date = '2025-04-01'\n",
    "end_date = '2025-05-01'\n",
    "START_DATE = '2025-04-01'\n",
    "END_DATE = '2025-05-01'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18ae2cfb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-02T04:10:27.208839Z",
     "start_time": "2025-05-02T04:10:18.875096Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "generatre_presence_score(MAIN_TOPIC, KEYWORDS, START_DATE, END_DATE, '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2299f55",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-02T07:57:36.641837Z",
     "start_time": "2025-05-02T07:57:05.116966Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from instagrapi import Client\n",
    "\n",
    "cl = Client()\n",
    "cl.login('moch_ariel', 'sukabumi03049567')\n",
    "user_id = cl.user_id_from_username('irdaniyp')\n",
    "medias = cl.user_medias(user_id, 20)  # Get the last 20 posts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbcde30c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-02T08:00:58.161005Z",
     "start_time": "2025-05-02T08:00:58.154042Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "medias[0].caption_text"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
